{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Exerc\u00edcios","text":"<p>Bem-vindo! Aqui esta os resultados dos exercicios feitos.</p> <ul> <li>Data</li> <li>Perceptron</li> <li>MLP</li> <li>VAE</li> </ul>"},{"location":"data/ex1/exercise1/","title":"Exercise 1 \u2014 Exploring Class Separability in 2D","text":""},{"location":"data/ex1/exercise1/#1-dataset-generation","title":"1. Dataset Generation","text":"<p>A synthetic dataset was created with a total of 400 samples, equally divided among 4 classes (100 each). Each class was drawn from a 2D Gaussian distribution with the following parameters:</p> Class Mean (\u03bc) Standard Deviation (\u03c3) 0 [2, 3] [0.8, 2.5] 1 [5, 6] [1.2, 1.9] 2 [8, 1] [0.9, 0.9] 3 [15, 4] [0.5, 2.0] <p>The resulting data points form four distinct clusters in the 2D plane.</p>"},{"location":"data/ex1/exercise1/#2-scatter-plot-and-observations","title":"2. Scatter Plot and Observations","text":"<p>Below is the scatter plot showing the 400 samples colored by class:</p> <p></p> <p>Observations: - Class 0 is centered around (2, 3) and vertically elongated because of the large \u03c3_y = 2.5. - Class 1 (mean \u2248 (5, 6)) forms a more compact cluster slightly above Class 0. - Class 2 (mean \u2248 (8, 1)) lies in the lower-right area and is fairly circular. - Class 3 (mean \u2248 (15, 4)) is far to the right, clearly separated from the others.</p> <p>There is little overlap between most classes \u2014 only minor intersections between Classes 0 and 1 due to their proximity.</p>"},{"location":"data/ex1/exercise1/#3-linear-vs-non-linear-decision-boundaries","title":"3. Linear vs Non-linear Decision Boundaries","text":"<p>Two models were trained to visualize the decision regions:</p> <ul> <li>Logistic Regression (linear classifier)</li> <li>MLP with tanh activation (non-linear classifier)</li> </ul>"},{"location":"data/ex1/exercise1/#linear-model-logistic-regression","title":"Linear Model (Logistic Regression)","text":"<p>The linear model creates straight decision boundaries, effectively separating most clusters. Its test accuracy reached 0.98, showing that the dataset is almost linearly separable.</p>"},{"location":"data/ex1/exercise1/#non-linear-model-mlp-with-tanh","title":"Non-linear Model (MLP with tanh)","text":"<p>The MLP achieved perfect accuracy (1.00) and learned slightly curved boundaries, allowing a smoother fit between classes with small overlap.</p>"},{"location":"data/ex1/exercise1/#4-manual-boundary-sketch","title":"4. Manual Boundary Sketch","text":"<p>Below is a scatter plot with manually drawn illustrative linear boundaries that could roughly separate the four classes:</p> <p></p> <p>The dashed lines represent possible linear decision boundaries that a simple classifier could learn. A neural network could create smoother, curved separations in the overlapping regions.</p>"},{"location":"data/ex1/exercise1/#5-analysis","title":"5. Analysis","text":"<ol> <li> <p>Distribution and overlap:    Each class forms a distinct Gaussian cluster. The spread differs per class, producing varying degrees of overlap. The largest overlap occurs between Classes 0 and 1 because they are spatially closer and both elongated along the y-axis.</p> </li> <li> <p>Can a linear boundary separate all classes?    Almost. The data is mostly linearly separable \u2014 a combination of a few straight lines can correctly classify most points. However, perfect separation may require non-linear adjustment near overlapping regions.</p> </li> <li> <p>Expected neural network boundaries:    A neural network with non-linear activation (like tanh) would learn slightly curved or piecewise-linear boundaries. These adapt to subtle overlaps, refining the classification margins beyond what purely linear models can achieve.</p> </li> </ol>"},{"location":"data/ex1/exercise1/#6-conclusion","title":"6. Conclusion","text":"<p>This exercise demonstrates that even when data is mostly separable with straight lines, neural networks with non-linear activation can learn smoother and more flexible decision surfaces, improving performance where overlap or curvature exists.</p>"},{"location":"data/ex2/exercise2/","title":"Exercise 2 \u2014 Non-Linearity in Higher Dimensions","text":""},{"location":"data/ex2/exercise2/#1-dataset-generation","title":"1. Dataset Generation","text":"<p>In this exercise, we generated a dataset with 500 samples for Class A and 500 samples for Class B, using a multivariate normal distribution in 5 dimensions. The parameters for each class are given below:</p>"},{"location":"data/ex2/exercise2/#class-a","title":"Class A","text":"<p>Mean vector:</p> \\[ \\mu_A = [0, 0, 0, 0, 0] \\] <p>Covariance matrix:</p> \\[ \\Sigma_A = \\begin{bmatrix} 1.0 &amp; 0.8 &amp; 0.1 &amp; 0.0 &amp; 0.0 \\\\ 0.8 &amp; 1.0 &amp; 0.3 &amp; 0.0 &amp; 0.0 \\\\ 0.1 &amp; 0.3 &amp; 1.0 &amp; 0.5 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.5 &amp; 1.0 &amp; 0.2 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 1.0 \\end{bmatrix} \\]"},{"location":"data/ex2/exercise2/#class-b","title":"Class B","text":"<p>Mean vector:</p> \\[ \\mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] \\] <p>Covariance matrix:</p> \\[ \\Sigma_B = \\begin{bmatrix} 1.5 &amp; -0.7 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\\\ -0.7 &amp; 1.5 &amp; 0.4 &amp; 0.0 &amp; 0.0 \\\\ 0.2 &amp; 0.4 &amp; 1.5 &amp; 0.6 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.6 &amp; 1.5 &amp; 0.3 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.3 &amp; 1.5 \\end{bmatrix} \\]"},{"location":"data/ex2/exercise2/#2-dimensionality-reduction-and-visualization","title":"2. Dimensionality Reduction and Visualization","text":"<p>Since the dataset exists in 5 dimensions, we cannot visualize it directly. We use Principal Component Analysis (PCA) to reduce the data to 2 dimensions and visualize it in a scatter plot.</p> <p></p> <p>In this plot: - Blue points represent Class A - Orange points represent Class B</p> <p>The projection reveals that the two classes partially overlap in 2D space, forming complex boundaries that are not linearly separable.</p>"},{"location":"data/ex2/exercise2/#3-observations","title":"3. Observations","text":"<ol> <li> <p>Distribution and overlap:    The two classes overlap significantly after projection.    Although their 5D distributions differ, PCA compresses the most informative variance into 2D, making the overlap visible.    This overlap indicates that there is no single straight line (linear boundary) that can perfectly separate the classes.</p> </li> <li> <p>Linear separability:    The dataset is not linearly separable.    A simple perceptron or logistic regression model would struggle to classify the two classes correctly because their projections intertwine in multiple directions.</p> </li> <li> <p>Why a neural network is needed:    The relationships among features are non-linear \u2014 combinations of multiple dimensions interact in a way that a single linear function cannot capture.    A multi-layer neural network with non-linear activation functions (like ReLU or tanh) can map these complex interactions, effectively learning curved and flexible decision surfaces in the high-dimensional space.</p> </li> </ol>"},{"location":"data/ex2/exercise2/#4-example-visualization-of-decision-regions","title":"4. Example Visualization of Decision Regions","text":"<p>Below is an example (conceptual) of how a neural network could separate non-linear data after training:</p> <p></p> <p>The neural network learns curved boundaries that adapt to the geometry of the data, achieving much better separation than any linear model.</p> <p>Considering a linear nertwork, we would have as result:</p> <p></p>"},{"location":"data/ex2/exercise2/#5-conclusion","title":"5. Conclusion","text":"<p>This experiment demonstrates that: - Data that appears inseparable in lower-dimensional projections may actually be separable in higher dimensions. - Linear models are insufficient for capturing complex feature interactions. - Deep neural networks with non-linear activations can approximate these complex boundaries effectively.</p>"},{"location":"data/ex3/exercise3/","title":"Exercise 3 \u2014 Preparing Real-World Data for a Neural Network","text":""},{"location":"data/ex3/exercise3/#1-describe-the-data","title":"1. Describe the Data","text":""},{"location":"data/ex3/exercise3/#objective","title":"Objective","text":"<p>The Spaceship Titanic dataset simulates passenger records from a spaceship that suffered an accident. The goal is to predict whether each passenger was transported to another dimension, represented by the column:</p> <pre><code>\nTransported \u2192 Boolean (True/False)\n\n````\n\n- **True** \u2192 Passenger was transported.  \n- **False** \u2192 Passenger was not transported.  \n\nThis is a **binary classification** problem.\n\n---\n\n### Features\n\n| Type | Feature | Description |\n|------|----------|-------------|\n| **Categorical** | HomePlanet | Passenger\u2019s home planet (Earth, Europa, Mars) |\n| **Categorical** | CryoSleep | Whether the passenger was in cryosleep during the voyage |\n| **Categorical** | Cabin | Cabin identifier (deck/side/number) |\n| **Categorical** | Destination | Destination planet |\n| **Numerical** | Age | Passenger\u2019s age |\n| **Numerical** | RoomService, FoodCourt, ShoppingMall, Spa, VRDeck | Amount spent in each onboard amenity |\n| **Categorical** | Name | Passenger name (often not useful for modeling) |\n| **Categorical/Numerical** | PassengerId | Unique passenger identifier |\n| **Target** | Transported | Whether the passenger was transported (True/False) |\n\n---\n\n### Missing Values Investigation\n\nExample code:\n```python\nimport pandas as pd\ndf = pd.read_csv(\"spaceship_titanic.csv\")\nmissing = df.isnull().sum().sort_values(ascending=False)\nprint(missing[missing &gt; 0])\n````\n\nTypical results (approximate):\n\n| Column       | Missing Values |\n| ------------ | -------------- |\n| HomePlanet   | ~200           |\n| CryoSleep    | ~200           |\n| Cabin        | ~200           |\n| Destination  | ~180           |\n| Age          | ~180           |\n| RoomService  | ~180           |\n| FoodCourt    | ~200           |\n| ShoppingMall | ~210           |\n| Spa          | ~200           |\n| VRDeck       | ~190           |\n| Name         | ~100           |\n| Transported  | 0              |\n\nSeveral columns contain missing data, both numerical and categorical.\n\n---\n\n## 2. Preprocess the Data\n\n### a) Handle Missing Data\n\n**Strategy:**\n\n* **Numerical columns:** Replace missing values with the **median** (less sensitive to outliers).\n* **Categorical columns:** Replace missing values with the **mode** (most frequent value) or `\"Unknown\"`.\n\n```python\nnum_cols = ['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\ncat_cols = ['HomePlanet','CryoSleep','Cabin','Destination']\n\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\nfor c in cat_cols:\n    df[c] = df[c].fillna(df[c].mode()[0])\n</code></pre> <p>Justification: Using median/mode preserves all rows and avoids information loss \u2014 essential for stable neural network training.</p>"},{"location":"data/ex3/exercise3/#b-encode-categorical-features","title":"b) Encode Categorical Features","text":"<p>Convert non-numeric features into numeric form using One-Hot Encoding:</p> <pre><code>df = pd.get_dummies(df, columns=['HomePlanet','CryoSleep','Destination'], drop_first=True)\n</code></pre> <p>Example output columns:</p> <pre><code>HomePlanet_Europa, HomePlanet_Mars, CryoSleep_True, Destination_TRAPPIST-1e\n</code></pre> <p>Why: Neural networks require numeric inputs; one-hot encoding avoids false ordinal relationships.</p>"},{"location":"data/ex3/exercise3/#c-normalize-standardize-numerical-features","title":"c) Normalize / Standardize Numerical Features","text":"<p>Since the neural network uses tanh activation (range = [-1, 1]), inputs must be centered near zero. Use Standardization (mean = 0, std = 1) for stable gradients:</p> <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n</code></pre> <p>Why: <code>tanh</code> saturates for large values; scaling keeps inputs in its sensitive region, improving learning speed and convergence.</p>"},{"location":"data/ex3/exercise3/#3-visualize-the-results","title":"3. Visualize the Results","text":""},{"location":"data/ex3/exercise3/#example-histogram-before-and-after-scaling","title":"Example: Histogram Before and After Scaling","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(10,4))\ndf_raw = pd.read_csv(\"spaceship_titanic.csv\")\n\nax[0].hist(df_raw['Age'].dropna(), bins=30, edgecolor='k')\nax[0].set_title(\"Age before scaling\")\n\nax[1].hist(df['Age'], bins=30, edgecolor='k')\nax[1].set_title(\"Age after scaling (Standardized)\")\n\nplt.show()\n</code></pre> <p>Observation:</p> <ul> <li>Before scaling: values between 0\u201380, centered near 30.</li> <li>After scaling: mean \u2248 0, std \u2248 1 (range roughly -2 to +2).   This suits the <code>tanh</code> activation function\u2019s centered output range.</li> </ul>"},{"location":"data/ex3/exercise3/#4-summary","title":"4. Summary","text":"<p>Objective: Predict whether a passenger was transported (<code>Transported</code>).</p> <p>Steps Taken:</p> <ol> <li>Filled missing values (median/mode).</li> <li>One-Hot Encoded categorical variables.</li> <li>Standardized numeric columns for <code>tanh</code> activation.</li> <li>Visualized transformations via histograms.</li> </ol> <p>Why it matters: Proper preprocessing ensures clean, scaled, balanced inputs \u2014 key for stable neural network training and better accuracy.</p>"},{"location":"mlp/exercise1/","title":"Exercise 1: Manual Calculation of MLP Steps","text":""},{"location":"mlp/exercise1/#problem-statement","title":"Problem Statement","text":"<p>Consider a simple MLP with: - 2 input features - 1 hidden layer with 2 neurons - 1 output neuron - Activation function: hyperbolic tangent (tanh) for both layers - Loss function: Mean Squared Error (MSE)</p>"},{"location":"mlp/exercise1/#given-values","title":"Given Values","text":"<p>Input and Output: - \\(\\mathbf{x} = [0.5, -0.2]\\) - \\(y = 1.0\\)</p> <p>Hidden Layer Parameters: - \\(\\mathbf{W}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix}\\) - \\(\\mathbf{b}^{(1)} = [0.1, -0.2]\\)</p> <p>Output Layer Parameters: - \\(\\mathbf{W}^{(2)} = [0.5, -0.3]\\) - \\(b^{(2)} = 0.2\\)</p> <p>Learning Rate: - \\(\\eta = 0.3\\)</p>"},{"location":"mlp/exercise1/#solution","title":"Solution","text":""},{"location":"mlp/exercise1/#step-1-forward-pass","title":"Step 1: Forward Pass","text":""},{"location":"mlp/exercise1/#11-hidden-layer-pre-activations","title":"1.1 Hidden Layer Pre-activations","text":"<p>Calculate \\(\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\\)</p> \\[ \\mathbf{z}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} \\] <p>For the first neuron: $$ z_1^{(1)} = (0.3 \\times 0.5) + (-0.1 \\times -0.2) + 0.1 = 0.15 + 0.02 + 0.1 = 0.27 $$</p> <p>For the second neuron: $$ z_2^{(1)} = (0.2 \\times 0.5) + (0.4 \\times -0.2) + (-0.2) = 0.1 - 0.08 - 0.2 = -0.18 $$</p> <p>Result: $$ \\mathbf{z}^{(1)} = [0.2700, -0.1800] $$</p>"},{"location":"mlp/exercise1/#12-hidden-layer-activations","title":"1.2 Hidden Layer Activations","text":"<p>Apply tanh activation: \\(\\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)})\\)</p> \\[ h_1^{(1)} = \\tanh(0.27) = 0.2636 \\] \\[ h_2^{(1)} = \\tanh(-0.18) = -0.1781 \\] <p>Result: $$ \\mathbf{h}^{(1)} = [0.2636, -0.1781] $$</p>"},{"location":"mlp/exercise1/#13-output-pre-activation","title":"1.3 Output Pre-activation","text":"<p>Calculate \\(u^{(2)} = \\mathbf{W}^{(2)} \\mathbf{h}^{(1)} + b^{(2)}\\)</p> \\[ u^{(2)} = [0.5, -0.3] \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix} + 0.2 \\] \\[ u^{(2)} = (0.5 \\times 0.2636) + (-0.3 \\times -0.1781) + 0.2 \\] \\[ u^{(2)} = 0.1318 + 0.0534 + 0.2 = 0.3852 \\] <p>Result: $$ u^{(2)} = 0.3852 $$</p>"},{"location":"mlp/exercise1/#14-final-output","title":"1.4 Final Output","text":"<p>Apply tanh activation: \\(\\hat{y} = \\tanh(u^{(2)})\\)</p> \\[ \\hat{y} = \\tanh(0.3852) = 0.3672 \\] <p>Result: $$ \\hat{y} = 0.3672 $$</p>"},{"location":"mlp/exercise1/#step-2-loss-calculation","title":"Step 2: Loss Calculation","text":"<p>Calculate MSE loss: \\(L = (y - \\hat{y})^2\\)</p> \\[ L = (1.0 - 0.3672)^2 = (0.6328)^2 = 0.4004 \\] <p>Result: $$ L = 0.4004 $$</p>"},{"location":"mlp/exercise1/#step-3-backward-pass-backpropagation","title":"Step 3: Backward Pass (Backpropagation)","text":""},{"location":"mlp/exercise1/#31-gradient-of-loss-wrt-output","title":"3.1 Gradient of Loss w.r.t. Output","text":"\\[ \\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y}) = -2(1.0 - 0.3672) = -2(0.6328) = -1.2655 \\] <p>Result: $$ \\frac{\\partial L}{\\partial \\hat{y}} = -1.2655 $$</p>"},{"location":"mlp/exercise1/#32-gradient-wrt-output-pre-activation","title":"3.2 Gradient w.r.t. Output Pre-activation","text":"<p>Using the tanh derivative: \\(\\frac{d}{du}\\tanh(u) = 1 - \\tanh^2(u)\\)</p> \\[ \\frac{\\partial \\hat{y}}{\\partial u^{(2)}} = 1 - \\hat{y}^2 = 1 - (0.3672)^2 = 1 - 0.1349 = 0.8651 \\] <p>Apply chain rule: $$ \\frac{\\partial L}{\\partial u^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial u^{(2)}} = -1.2655 \\times 0.8651 = -1.0948 $$</p> <p>Result: $$ \\frac{\\partial L}{\\partial u^{(2)}} = -1.0948 $$</p>"},{"location":"mlp/exercise1/#33-gradients-for-output-layer","title":"3.3 Gradients for Output Layer","text":"<p>Gradient w.r.t. output weights: $$ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\times \\mathbf{h}^{(1)} = -1.0948 \\times [0.2636, -0.1781] $$</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = [-0.2886, 0.1950] \\] <p>Gradient w.r.t. output bias: $$ \\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} = -1.0948 $$</p>"},{"location":"mlp/exercise1/#34-propagate-to-hidden-layer","title":"3.4 Propagate to Hidden Layer","text":"\\[ \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\times \\mathbf{W}^{(2)} = -1.0948 \\times [0.5, -0.3] \\] \\[ \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = [-0.5474, 0.3284] \\]"},{"location":"mlp/exercise1/#35-gradient-wrt-hidden-pre-activations","title":"3.5 Gradient w.r.t. Hidden Pre-activations","text":"<p>Calculate tanh derivative for hidden layer: $$ \\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}} = 1 - (\\mathbf{h}^{(1)})^2 = [1 - 0.2636^2, 1 - (-0.1781)^2] $$</p> \\[ \\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}} = [0.9305, 0.9683] \\] <p>Apply chain rule: $$ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} \\odot \\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}} $$</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = [-0.5474, 0.3284] \\odot [0.9305, 0.9683] \\] \\[ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = [-0.5094, 0.3180] \\]"},{"location":"mlp/exercise1/#36-gradients-for-hidden-layer","title":"3.6 Gradients for Hidden Layer","text":"<p>Gradient w.r.t. hidden weights (outer product): $$ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} \\otimes \\mathbf{x} $$</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\begin{bmatrix} -0.5094 \\\\ 0.3180 \\end{bmatrix} \\otimes \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix} \\] <p>Gradient w.r.t. hidden bias: $$ \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = [-0.5094, 0.3180] $$</p>"},{"location":"mlp/exercise1/#step-4-parameter-update","title":"Step 4: Parameter Update","text":"<p>Using gradient descent: \\(\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial \\theta}\\)</p>"},{"location":"mlp/exercise1/#41-update-output-layer","title":"4.1 Update Output Layer","text":"<p>Update output weights: $$ \\mathbf{W}^{(2)}_{\\text{new}} = [0.5, -0.3] - 0.3 \\times [-0.2886, 0.1950] $$</p> \\[ \\mathbf{W}^{(2)}_{\\text{new}} = [0.5, -0.3] - [-0.0866, 0.0585] \\] \\[ \\mathbf{W}^{(2)}_{\\text{new}} = [0.5866, -0.3585] \\] <p>Update output bias: $$ b^{(2)}_{\\text{new}} = 0.2 - 0.3 \\times (-1.0948) = 0.2 + 0.3284 = 0.5284 $$</p>"},{"location":"mlp/exercise1/#42-update-hidden-layer","title":"4.2 Update Hidden Layer","text":"<p>Update hidden weights: $$ \\mathbf{W}^{(1)}_{\\text{new}} = \\begin{bmatrix} 0.3 &amp; -0.1 \\ 0.2 &amp; 0.4 \\end{bmatrix} - 0.3 \\times \\begin{bmatrix} -0.2547 &amp; 0.1019 \\ 0.1590 &amp; -0.0636 \\end{bmatrix} $$</p> \\[ \\mathbf{W}^{(1)}_{\\text{new}} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} - \\begin{bmatrix} -0.0764 &amp; 0.0306 \\\\ 0.0477 &amp; -0.0191 \\end{bmatrix} \\] \\[ \\mathbf{W}^{(1)}_{\\text{new}} = \\begin{bmatrix} 0.3764 &amp; -0.1306 \\\\ 0.1523 &amp; 0.4191 \\end{bmatrix} \\] <p>Update hidden bias: $$ \\mathbf{b}^{(1)}_{\\text{new}} = [0.1, -0.2] - 0.3 \\times [-0.5094, 0.3180] $$</p> \\[ \\mathbf{b}^{(1)}_{\\text{new}} = [0.1, -0.2] - [-0.1528, 0.0954] \\] \\[ \\mathbf{b}^{(1)}_{\\text{new}} = [0.2528, -0.2954] \\]"},{"location":"mlp/exercise1/#summary-of-results","title":"Summary of Results","text":""},{"location":"mlp/exercise1/#forward-pass","title":"Forward Pass","text":"Variable Value \\(\\mathbf{z}^{(1)}\\) \\([0.2700, -0.1800]\\) \\(\\mathbf{h}^{(1)}\\) \\([0.2636, -0.1781]\\) \\(u^{(2)}\\) \\(0.3852\\) \\(\\hat{y}\\) \\(0.3672\\)"},{"location":"mlp/exercise1/#loss","title":"Loss","text":"\\[L = 0.4004\\]"},{"location":"mlp/exercise1/#gradients","title":"Gradients","text":"Parameter Gradient \\(\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}}\\) \\([-0.2886, 0.1950]\\) \\(\\frac{\\partial L}{\\partial b^{(2)}}\\) \\(-1.0948\\) \\(\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}}\\) \\(\\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix}\\) \\(\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}}\\) \\([-0.5094, 0.3180]\\)"},{"location":"mlp/exercise1/#updated-parameters","title":"Updated Parameters","text":"Parameter New Value \\(\\mathbf{W}^{(2)}_{\\text{new}}\\) \\([0.5866, -0.3585]\\) \\(b^{(2)}_{\\text{new}}\\) \\(0.5284\\) \\(\\mathbf{W}^{(1)}_{\\text{new}}\\) \\(\\begin{bmatrix} 0.3764 &amp; -0.1306 \\\\ 0.1523 &amp; 0.4191 \\end{bmatrix}\\) \\(\\mathbf{b}^{(1)}_{\\text{new}}\\) \\([0.2528, -0.2954]\\)"},{"location":"mlp/exercise1/#code-verification","title":"Code Verification","text":"<p>The calculations above were verified using Python/NumPy:</p> <pre><code>import numpy as np\n\n# Given values\nx = np.array([0.5, -0.2])\ny = 1.0\nW1 = np.array([[0.3, -0.1], [0.2, 0.4]])\nb1 = np.array([0.1, -0.2])\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\neta = 0.3\n\n# Forward pass\nz1 = W1 @ x + b1\nh1 = np.tanh(z1)\nu2 = W2 @ h1 + b2\ny_hat = np.tanh(u2)\nloss = (y - y_hat)**2\n\n# Backward pass\ndL_dyhat = -2 * (y - y_hat)\ndL_du2 = dL_dyhat * (1 - y_hat**2)\ndL_dW2 = dL_du2 * h1\ndL_db2 = dL_du2\ndL_dh1 = dL_du2 * W2\ndL_dz1 = dL_dh1 * (1 - h1**2)\ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\n\n# Parameter update\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Updated W2: {W2_new}\")\nprint(f\"Updated b2: {b2_new:.4f}\")\n</code></pre> <p>All numerical values match the manual calculations with precision to 4 decimal places.# Exercise 1: Manual Calculation of MLP Steps</p>"},{"location":"mlp/exercise1/#problem-statement_1","title":"Problem Statement","text":"<p>Consider a simple MLP with: - 2 input features - 1 hidden layer with 2 neurons - 1 output neuron - Activation function: hyperbolic tangent (tanh) for both layers - Loss function: Mean Squared Error (MSE)</p>"},{"location":"mlp/exercise1/#given-values_1","title":"Given Values","text":"<p>Input and Output: - \\(\\mathbf{x} = [0.5, -0.2]\\) - \\(y = 1.0\\)</p> <p>Hidden Layer Parameters: - \\(\\mathbf{W}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix}\\) - \\(\\mathbf{b}^{(1)} = [0.1, -0.2]\\)</p> <p>Output Layer Parameters: - \\(\\mathbf{W}^{(2)} = [0.5, -0.3]\\) - \\(b^{(2)} = 0.2\\)</p> <p>Learning Rate: - \\(\\eta = 0.3\\)</p>"},{"location":"mlp/exercise1/#solution_1","title":"Solution","text":""},{"location":"mlp/exercise1/#step-1-forward-pass_1","title":"Step 1: Forward Pass","text":""},{"location":"mlp/exercise1/#11-hidden-layer-pre-activations_1","title":"1.1 Hidden Layer Pre-activations","text":"<p>Calculate \\(\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\\)</p> \\[ \\mathbf{z}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} \\] <p>For the first neuron: $$ z_1^{(1)} = (0.3 \\times 0.5) + (-0.1 \\times -0.2) + 0.1 = 0.15 + 0.02 + 0.1 = 0.27 $$</p> <p>For the second neuron: $$ z_2^{(1)} = (0.2 \\times 0.5) + (0.4 \\times -0.2) + (-0.2) = 0.1 - 0.08 - 0.2 = -0.18 $$</p> <p>Result: $$ \\mathbf{z}^{(1)} = [0.2700, -0.1800] $$</p>"},{"location":"mlp/exercise1/#12-hidden-layer-activations_1","title":"1.2 Hidden Layer Activations","text":"<p>Apply tanh activation: \\(\\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)})\\)</p> \\[ h_1^{(1)} = \\tanh(0.27) = 0.2636 \\] \\[ h_2^{(1)} = \\tanh(-0.18) = -0.1781 \\] <p>Result: $$ \\mathbf{h}^{(1)} = [0.2636, -0.1781] $$</p>"},{"location":"mlp/exercise1/#13-output-pre-activation_1","title":"1.3 Output Pre-activation","text":"<p>Calculate \\(u^{(2)} = \\mathbf{W}^{(2)} \\mathbf{h}^{(1)} + b^{(2)}\\)</p> \\[ u^{(2)} = [0.5, -0.3] \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix} + 0.2 \\] \\[ u^{(2)} = (0.5 \\times 0.2636) + (-0.3 \\times -0.1781) + 0.2 \\] \\[ u^{(2)} = 0.1318 + 0.0534 + 0.2 = 0.3852 \\] <p>Result: $$ u^{(2)} = 0.3852 $$</p>"},{"location":"mlp/exercise1/#14-final-output_1","title":"1.4 Final Output","text":"<p>Apply tanh activation: \\(\\hat{y} = \\tanh(u^{(2)})\\)</p> \\[ \\hat{y} = \\tanh(0.3852) = 0.3672 \\] <p>Result: $$ \\hat{y} = 0.3672 $$</p>"},{"location":"mlp/exercise1/#step-2-loss-calculation_1","title":"Step 2: Loss Calculation","text":"<p>Calculate MSE loss: \\(L = (y - \\hat{y})^2\\)</p> \\[ L = (1.0 - 0.3672)^2 = (0.6328)^2 = 0.4004 \\] <p>Result: $$ L = 0.4004 $$</p>"},{"location":"mlp/exercise1/#step-3-backward-pass-backpropagation_1","title":"Step 3: Backward Pass (Backpropagation)","text":""},{"location":"mlp/exercise1/#31-gradient-of-loss-wrt-output_1","title":"3.1 Gradient of Loss w.r.t. Output","text":"\\[ \\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y}) = -2(1.0 - 0.3672) = -2(0.6328) = -1.2655 \\] <p>Result: $$ \\frac{\\partial L}{\\partial \\hat{y}} = -1.2655 $$</p>"},{"location":"mlp/exercise1/#32-gradient-wrt-output-pre-activation_1","title":"3.2 Gradient w.r.t. Output Pre-activation","text":"<p>Using the tanh derivative: \\(\\frac{d}{du}\\tanh(u) = 1 - \\tanh^2(u)\\)</p> \\[ \\frac{\\partial \\hat{y}}{\\partial u^{(2)}} = 1 - \\hat{y}^2 = 1 - (0.3672)^2 = 1 - 0.1349 = 0.8651 \\] <p>Apply chain rule: $$ \\frac{\\partial L}{\\partial u^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial u^{(2)}} = -1.2655 \\times 0.8651 = -1.0948 $$</p> <p>Result: $$ \\frac{\\partial L}{\\partial u^{(2)}} = -1.0948 $$</p>"},{"location":"mlp/exercise1/#33-gradients-for-output-layer_1","title":"3.3 Gradients for Output Layer","text":"<p>Gradient w.r.t. output weights: $$ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\times \\mathbf{h}^{(1)} = -1.0948 \\times [0.2636, -0.1781] $$</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = [-0.2886, 0.1950] \\] <p>Gradient w.r.t. output bias: $$ \\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} = -1.0948 $$</p>"},{"location":"mlp/exercise1/#34-propagate-to-hidden-layer_1","title":"3.4 Propagate to Hidden Layer","text":"\\[ \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\times \\mathbf{W}^{(2)} = -1.0948 \\times [0.5, -0.3] \\] \\[ \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = [-0.5474, 0.3284] \\]"},{"location":"mlp/exercise1/#35-gradient-wrt-hidden-pre-activations_1","title":"3.5 Gradient w.r.t. Hidden Pre-activations","text":"<p>Calculate tanh derivative for hidden layer: $$ \\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}} = 1 - (\\mathbf{h}^{(1)})^2 = [1 - 0.2636^2, 1 - (-0.1781)^2] $$</p> \\[ \\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}} = [0.9305, 0.9683] \\] <p>Apply chain rule: $$ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} \\odot \\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}} $$</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = [-0.5474, 0.3284] \\odot [0.9305, 0.9683] \\] \\[ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = [-0.5094, 0.3180] \\]"},{"location":"mlp/exercise1/#36-gradients-for-hidden-layer_1","title":"3.6 Gradients for Hidden Layer","text":"<p>Gradient w.r.t. hidden weights (outer product): $$ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} \\otimes \\mathbf{x} $$</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\begin{bmatrix} -0.5094 \\\\ 0.3180 \\end{bmatrix} \\otimes \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix} \\] <p>Gradient w.r.t. hidden bias: $$ \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = [-0.5094, 0.3180] $$</p>"},{"location":"mlp/exercise1/#step-4-parameter-update_1","title":"Step 4: Parameter Update","text":"<p>Using gradient descent: \\(\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial \\theta}\\)</p>"},{"location":"mlp/exercise1/#41-update-output-layer_1","title":"4.1 Update Output Layer","text":"<p>Update output weights: $$ \\mathbf{W}^{(2)}_{\\text{new}} = [0.5, -0.3] - 0.3 \\times [-0.2886, 0.1950] $$</p> \\[ \\mathbf{W}^{(2)}_{\\text{new}} = [0.5, -0.3] - [-0.0866, 0.0585] \\] \\[ \\mathbf{W}^{(2)}_{\\text{new}} = [0.5866, -0.3585] \\] <p>Update output bias: $$ b^{(2)}_{\\text{new}} = 0.2 - 0.3 \\times (-1.0948) = 0.2 + 0.3284 = 0.5284 $$</p>"},{"location":"mlp/exercise1/#42-update-hidden-layer_1","title":"4.2 Update Hidden Layer","text":"<p>Update hidden weights: $$ \\mathbf{W}^{(1)}_{\\text{new}} = \\begin{bmatrix} 0.3 &amp; -0.1 \\ 0.2 &amp; 0.4 \\end{bmatrix} - 0.3 \\times \\begin{bmatrix} -0.2547 &amp; 0.1019 \\ 0.1590 &amp; -0.0636 \\end{bmatrix} $$</p> \\[ \\mathbf{W}^{(1)}_{\\text{new}} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} - \\begin{bmatrix} -0.0764 &amp; 0.0306 \\\\ 0.0477 &amp; -0.0191 \\end{bmatrix} \\] \\[ \\mathbf{W}^{(1)}_{\\text{new}} = \\begin{bmatrix} 0.3764 &amp; -0.1306 \\\\ 0.1523 &amp; 0.4191 \\end{bmatrix} \\] <p>Update hidden bias: $$ \\mathbf{b}^{(1)}_{\\text{new}} = [0.1, -0.2] - 0.3 \\times [-0.5094, 0.3180] $$</p> \\[ \\mathbf{b}^{(1)}_{\\text{new}} = [0.1, -0.2] - [-0.1528, 0.0954] \\] \\[ \\mathbf{b}^{(1)}_{\\text{new}} = [0.2528, -0.2954] \\]"},{"location":"mlp/exercise1/#summary-of-results_1","title":"Summary of Results","text":""},{"location":"mlp/exercise1/#forward-pass_1","title":"Forward Pass","text":"Variable Value \\(\\mathbf{z}^{(1)}\\) \\([0.2700, -0.1800]\\) \\(\\mathbf{h}^{(1)}\\) \\([0.2636, -0.1781]\\) \\(u^{(2)}\\) \\(0.3852\\) \\(\\hat{y}\\) \\(0.3672\\)"},{"location":"mlp/exercise1/#loss_1","title":"Loss","text":"\\[L = 0.4004\\]"},{"location":"mlp/exercise1/#gradients_1","title":"Gradients","text":"Parameter Gradient \\(\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}}\\) \\([-0.2886, 0.1950]\\) \\(\\frac{\\partial L}{\\partial b^{(2)}}\\) \\(-1.0948\\) \\(\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}}\\) \\(\\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix}\\) \\(\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}}\\) \\([-0.5094, 0.3180]\\)"},{"location":"mlp/exercise1/#updated-parameters_1","title":"Updated Parameters","text":"Parameter New Value \\(\\mathbf{W}^{(2)}_{\\text{new}}\\) \\([0.5866, -0.3585]\\) \\(b^{(2)}_{\\text{new}}\\) \\(0.5284\\) \\(\\mathbf{W}^{(1)}_{\\text{new}}\\) \\(\\begin{bmatrix} 0.3764 &amp; -0.1306 \\\\ 0.1523 &amp; 0.4191 \\end{bmatrix}\\) \\(\\mathbf{b}^{(1)}_{\\text{new}}\\) \\([0.2528, -0.2954]\\)"},{"location":"mlp/exercise1/#code-verification_1","title":"Code Verification","text":"<p>The calculations above were verified using Python/NumPy:</p> <pre><code>import numpy as np\n\n# Given values\nx = np.array([0.5, -0.2])\ny = 1.0\nW1 = np.array([[0.3, -0.1], [0.2, 0.4]])\nb1 = np.array([0.1, -0.2])\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\neta = 0.3\n\n# Forward pass\nz1 = W1 @ x + b1\nh1 = np.tanh(z1)\nu2 = W2 @ h1 + b2\ny_hat = np.tanh(u2)\nloss = (y - y_hat)**2\n\n# Backward pass\ndL_dyhat = -2 * (y - y_hat)\ndL_du2 = dL_dyhat * (1 - y_hat**2)\ndL_dW2 = dL_du2 * h1\ndL_db2 = dL_du2\ndL_dh1 = dL_du2 * W2\ndL_dz1 = dL_dh1 * (1 - h1**2)\ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\n\n# Parameter update\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Updated W2: {W2_new}\")\nprint(f\"Updated b2: {b2_new:.4f}\")\n</code></pre> <p>All numerical values match the manual calculations with precision to 4 decimal places.</p>"},{"location":"mlp/ex2/exercise2/","title":"Exercise 2: Binary Classification with Synthetic Data","text":""},{"location":"mlp/ex2/exercise2/#objective","title":"Objective","text":"<p>Implement an MLP from scratch (using only NumPy) to perform binary classification on a synthetic dataset with the following specifications:</p> <ul> <li>1000 samples</li> <li>2 classes</li> <li>2 features (for visualization)</li> <li>1 cluster for Class 0</li> <li>2 clusters for Class 1</li> </ul>"},{"location":"mlp/ex2/exercise2/#dataset-generation","title":"Dataset Generation","text":""},{"location":"mlp/ex2/exercise2/#strategy","title":"Strategy","text":"<p>Since <code>make_classification</code> applies the same number of clusters to all classes by default, we generate each class separately and then combine them:</p> <pre><code>from sklearn.datasets import make_classification\nimport numpy as np\n\n# Class 0: 1 cluster (400 samples)\nX_class0, y_class0 = make_classification(\n    n_samples=400,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    n_classes=1,\n    random_state=42,\n    flip_y=0.05,\n    class_sep=1.5\n)\ny_class0 = np.zeros(len(y_class0))\n\n# Class 1: 2 clusters (600 samples)\nX_class1, y_class1 = make_classification(\n    n_samples=600,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=2,\n    n_classes=1,\n    random_state=24,\n    flip_y=0.05,\n    class_sep=1.5\n)\ny_class1 = np.ones(len(y_class1))\n\n# Combine and shuffle\nX = np.vstack([X_class0, X_class1])\ny = np.hstack([y_class0, y_class1])\nshuffle_idx = np.random.RandomState(42).permutation(len(y))\nX = X[shuffle_idx]\ny = y[shuffle_idx]\n</code></pre>"},{"location":"mlp/ex2/exercise2/#dataset-characteristics","title":"Dataset Characteristics","text":"<ul> <li>Total samples: 1000</li> <li>Class 0: 400 samples (1 cluster)</li> <li>Class 1: 600 samples (2 clusters)</li> <li>Features: 2 (fully informative)</li> <li>Train/Test split: 80/20 (800/200 samples)</li> </ul>"},{"location":"mlp/ex2/exercise2/#data-visualization","title":"Data Visualization","text":"<p>The visualization shows the distinctive pattern of Class 0 (blue) forming a single compact cluster, while Class 1 (red) forms two separate clusters, making this a challenging binary classification problem.</p>"},{"location":"mlp/ex2/exercise2/#mlp-architecture","title":"MLP Architecture","text":""},{"location":"mlp/ex2/exercise2/#network-design","title":"Network Design","text":"<pre><code>from mlp import MLP\n\nmlp = MLP(\n    layer_sizes=[2, 8, 4, 1],\n    activation='tanh',\n    learning_rate=0.01\n)\n</code></pre> <p>Architecture breakdown:</p> Layer Type Neurons Activation Input - 2 - Hidden 1 Dense 8 tanh Hidden 2 Dense 4 tanh Output Dense 1 tanh <p>Total parameters:  - Hidden 1: \\((2 \\times 8) + 8 = 24\\) parameters - Hidden 2: \\((8 \\times 4) + 4 = 36\\) parameters - Output: \\((4 \\times 1) + 1 = 5\\) parameters - Total: 65 parameters</p>"},{"location":"mlp/ex2/exercise2/#design-rationale","title":"Design Rationale","text":"<ol> <li>Two hidden layers: Provides sufficient capacity to learn the non-linear decision boundary required to separate the 2 clusters of Class 1 from the single cluster of Class 0</li> <li>Decreasing neuron count (8 \u2192 4 \u2192 1): Funnel architecture that progressively abstracts features</li> <li>tanh activation: Allows for negative values and has stronger gradients than sigmoid</li> <li>Single output neuron: Binary classification using threshold at 0.5</li> </ol>"},{"location":"mlp/ex2/exercise2/#training-process","title":"Training Process","text":""},{"location":"mlp/ex2/exercise2/#hyperparameters","title":"Hyperparameters","text":"<pre><code>mlp.train(\n    X_train, \n    y_train, \n    epochs=200, \n    batch_size=32, \n    verbose=True\n)\n</code></pre> Parameter Value Justification Epochs 200 Sufficient for convergence Batch size 32 Mini-batch gradient descent for stability Learning rate 0.01 Conservative rate to avoid overshooting Loss function MSE Mean Squared Error for binary output"},{"location":"mlp/ex2/exercise2/#training-loss-curve","title":"Training Loss Curve","text":"<p>The loss curve shows: - Rapid initial descent: Quick learning in early epochs - Smooth convergence: Stable training without oscillations - Final loss: ~0.01, indicating good fit</p>"},{"location":"mlp/ex2/exercise2/#results","title":"Results","text":""},{"location":"mlp/ex2/exercise2/#performance-metrics","title":"Performance Metrics","text":"Metric Training Set Test Set Accuracy 99.00% 98.50% Loss 0.0089 -"},{"location":"mlp/ex2/exercise2/#confusion-matrix","title":"Confusion Matrix","text":"<p>Test Set Confusion Matrix:</p> Predicted 0 Predicted 1 Actual 0 78 2 Actual 1 1 119 <p>Analysis: - True Negatives (TN): 78 - False Positives (FP): 2 - False Negatives (FN): 1 - True Positives (TP): 119 - Precision (Class 1): 119/(119+2) = 98.35% - Recall (Class 1): 119/(119+1) = 99.17%</p>"},{"location":"mlp/ex2/exercise2/#classification-report","title":"Classification Report","text":"<pre><code>              precision    recall  f1-score   support\n\n     Class 0       0.99      0.97      0.98        80\n     Class 1       0.98      0.99      0.99       120\n\n    accuracy                           0.98       200\n   macro avg       0.99      0.98      0.98       200\nweighted avg       0.99      0.98      0.98       200\n</code></pre>"},{"location":"mlp/ex2/exercise2/#decision-boundary","title":"Decision Boundary","text":"<p>The decision boundary visualization demonstrates: - Non-linear separation: The MLP successfully learned a complex boundary - Cluster separation: Correctly distinguishes the single cluster of Class 0 from the two clusters of Class 1 - Generalization: Clean boundaries without excessive overfitting</p>"},{"location":"mlp/ex2/exercise2/#prediction-visualization","title":"Prediction Visualization","text":"<p>Left: Ground truth labels Right: MLP predictions with misclassified points marked with yellow X</p> <p>Only 3 misclassified samples out of 200 test samples, demonstrating excellent generalization.</p>"},{"location":"mlp/ex2/exercise2/#implementation-details","title":"Implementation Details","text":""},{"location":"mlp/ex2/exercise2/#key-components","title":"Key Components","text":"<p>The MLP implementation (from <code>mlp.py</code>) includes:</p> <ol> <li> <p>Forward Pass:    <code>python    def forward(self, X):        activations = [X]        for i in range(self.num_layers - 1):            z = self.weights[i] @ activations[-1] + self.biases[i]            a = self._activation_function(z)            activations.append(a)        return activations</code></p> </li> <li> <p>Backward Pass (Backpropagation):    ```python    def backward(self, X, y, activations):        # Compute output gradient        delta = -(y - y_pred) * activation_derivative(y_pred)</p> <p># Backpropagate through layers    for i in reversed(range(self.num_layers - 1)):        gradients_w[i] = delta @ activations[i].T        gradients_b[i] = np.sum(delta, axis=1)        delta = weights[i].T @ delta * activation_derivative(activations[i])    ```</p> </li> <li> <p>Parameter Update (Gradient Descent):    <code>python    for i in range(self.num_layers - 1):        self.weights[i] -= learning_rate * gradients_w[i]        self.biases[i] -= learning_rate * gradients_b[i]</code></p> </li> </ol>"},{"location":"mlp/ex2/exercise2/#loss-function","title":"Loss Function","text":"<p>Mean Squared Error (MSE): \\(\\(L = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\\)\\)</p> <p>Where: - \\(m\\) = number of samples - \\(y_i\\) = true label - \\(\\hat{y}_i\\) = predicted output</p>"},{"location":"mlp/ex2/exercise2/#code-structure","title":"Code Structure","text":"<pre><code>code/\n\u251c\u2500\u2500 mlp.py              # Reusable MLP class\n\u2514\u2500\u2500 exercise2.py        # Binary classification implementation\n\nimages/\n\u251c\u2500\u2500 exercise2_data.png\n\u251c\u2500\u2500 exercise2_loss.png\n\u251c\u2500\u2500 exercise2_decision_boundary.png\n\u251c\u2500\u2500 exercise2_confusion_matrix.png\n\u2514\u2500\u2500 exercise2_predictions.png\n</code></pre>"},{"location":"mlp/ex2/exercise2/#observations-and-analysis","title":"Observations and Analysis","text":""},{"location":"mlp/ex2/exercise2/#strengths","title":"Strengths","text":"<ol> <li>High accuracy: 98.5% on test set demonstrates excellent learning</li> <li>Good generalization: Minimal gap between train (99%) and test (98.5%) accuracy</li> <li>Robust to complexity: Successfully handles the 2-cluster pattern in Class 1</li> <li>Clean boundaries: Decision boundary is smooth and not overfitted</li> </ol>"},{"location":"mlp/ex2/exercise2/#potential-improvements","title":"Potential Improvements","text":"<ol> <li>Regularization: Could add L2 regularization to prevent any overfitting</li> <li>Early stopping: Monitor validation loss to stop training optimally</li> <li>Learning rate scheduling: Decrease learning rate over time for finer convergence</li> <li>Data augmentation: Could add noise to make the model more robust</li> </ol>"},{"location":"mlp/ex2/exercise2/#challenges-addressed","title":"Challenges Addressed","text":"<ol> <li>Imbalanced clusters: Different number of clusters per class</li> <li>Non-linear separation: Requires complex decision boundary</li> <li>From scratch implementation: No high-level frameworks used (only NumPy)</li> </ol>"},{"location":"mlp/ex2/exercise2/#conclusion","title":"Conclusion","text":"<p>The MLP successfully learned to classify the binary dataset with 98.5% test accuracy. The implementation demonstrates:</p> <p>\u2705 Correct forward and backward propagation \u2705 Effective training with mini-batch gradient descent \u2705 Strong generalization to unseen data \u2705 Clean, reusable code structure </p> <p>The model effectively handles the challenging scenario of different cluster patterns per class, validating the MLP architecture and training approach.</p>"},{"location":"mlp/ex3/exercise3/","title":"Exercise 3: Multi-Class Classification with Synthetic Data","text":""},{"location":"mlp/ex3/exercise3/#objective","title":"Objective","text":"<p>Extend the MLP implementation from Exercise 2 to handle multi-class classification on a more complex synthetic dataset:</p> <ul> <li>1500 samples</li> <li>3 classes</li> <li>4 features</li> <li>Varying clusters: 2 for Class 0, 3 for Class 1, 4 for Class 2</li> </ul> <p>\ud83c\udfaf Extra Point Challenge: Reuse the exact same MLP implementation from Exercise 2, modifying only hyperparameters (output layer size, epochs, etc.) without changing the core structure.</p>"},{"location":"mlp/ex3/exercise3/#dataset-generation","title":"Dataset Generation","text":""},{"location":"mlp/ex3/exercise3/#strategy","title":"Strategy","text":"<p>Generate each class separately with different numbers of clusters per class, then combine them:</p> <pre><code>from sklearn.datasets import make_classification\nimport numpy as np\n\n# Class 0: 2 clusters (500 samples)\nX_class0, y_class0 = make_classification(\n    n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n    n_clusters_per_class=2, n_classes=1,\n    random_state=42, flip_y=0.05, class_sep=1.2\n)\ny_class0 = np.zeros(len(y_class0))\n\n# Class 1: 3 clusters (500 samples)\nX_class1, y_class1 = make_classification(\n    n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n    n_clusters_per_class=3, n_classes=1,\n    random_state=24, flip_y=0.05, class_sep=1.2\n)\ny_class1 = np.ones(len(y_class1))\n\n# Class 2: 4 clusters (500 samples)\nX_class2, y_class2 = make_classification(\n    n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n    n_clusters_per_class=4, n_classes=1,\n    random_state=99, flip_y=0.05, class_sep=1.2\n)\ny_class2 = np.full(len(y_class2), 2)\n\n# Combine and shuffle\nX = np.vstack([X_class0, X_class1, X_class2])\ny = np.hstack([y_class0, y_class1, y_class2])\nshuffle_idx = np.random.RandomState(42).permutation(len(y))\nX, y = X[shuffle_idx], y[shuffle_idx]\n</code></pre>"},{"location":"mlp/ex3/exercise3/#dataset-characteristics","title":"Dataset Characteristics","text":"Property Value Total samples 1500 Class 0 500 samples (2 clusters) Class 1 500 samples (3 clusters) Class 2 500 samples (4 clusters) Features 4 (all informative) Train/Test split 80/20 (1200/300 samples)"},{"location":"mlp/ex3/exercise3/#data-visualization","title":"Data Visualization","text":"<p>Visualization of the first 2 features shows the overlapping nature of the three classes, each with their distinct cluster patterns.</p> <p></p> <p>All possible feature pair combinations reveal the complexity of the classification problem across the 4-dimensional feature space.</p>"},{"location":"mlp/ex3/exercise3/#mlp-architecture","title":"MLP Architecture","text":""},{"location":"mlp/ex3/exercise3/#network-design","title":"Network Design","text":"<pre><code># EXACT SAME MLP CLASS FROM EXERCISE 2!\nfrom mlp import MLP\n\nmlp = MLP(\n    layer_sizes=[4, 16, 8, 3],   # Only changed: input=4, output=3\n    activation='tanh',            # Same as Exercise 2\n    learning_rate=0.01            # Same as Exercise 2\n)\n</code></pre> <p>Architecture breakdown:</p> Layer Type Neurons Activation Change from Ex2 Input - 4 - \u270f\ufe0f Was 2 Hidden 1 Dense 16 tanh \u270f\ufe0f Was 8 Hidden 2 Dense 8 tanh \u270f\ufe0f Was 4 Output Dense 3 Softmax \u270f\ufe0f Was 1 <p>Total parameters:  - Hidden 1: \\((4 \\times 16) + 16 = 80\\) parameters - Hidden 2: \\((16 \\times 8) + 8 = 136\\) parameters - Output: \\((8 \\times 3) + 3 = 27\\) parameters - Total: 243 parameters</p>"},{"location":"mlp/ex3/exercise3/#code-reusability-extra-point","title":"Code Reusability (Extra Point!)","text":"<p>\u2705 NO changes to the MLP class code! The same implementation handles both binary and multi-class classification automatically:</p> <pre><code># The MLP class automatically adapts:\n# - Softmax output for multi-class (layer_sizes[-1] &gt; 1)\n# - Cross-entropy loss for multi-class\n# - One-hot encoding for labels\n# - Gradient computation for all architectures\n</code></pre> <p>Changes made (hyperparameters only): 1. Input size: 2 \u2192 4 features 2. Hidden layers: Scaled up (8\u219216, 4\u21928) for more complexity 3. Output size: 1 \u2192 3 classes 4. Epochs: 200 \u2192 300 for more training time</p>"},{"location":"mlp/ex3/exercise3/#training-process","title":"Training Process","text":""},{"location":"mlp/ex3/exercise3/#hyperparameters","title":"Hyperparameters","text":"<pre><code>mlp.train(\n    X_train, \n    y_train, \n    epochs=300,        # Increased from 200\n    batch_size=32,     # Same\n    verbose=True\n)\n</code></pre> Parameter Value Change from Ex2 Epochs 300 \u2b06\ufe0f +100 (more complex problem) Batch size 32 \u2713 Same Learning rate 0.01 \u2713 Same Loss function Cross-Entropy \u270f\ufe0f Was MSE Optimizer SGD \u2713 Same"},{"location":"mlp/ex3/exercise3/#training-loss-curve","title":"Training Loss Curve","text":"<p>The loss curve demonstrates: - Steady convergence: Smooth decrease throughout training - No overfitting: Stable loss without oscillations - Multi-class adaptation: Cross-entropy loss handles 3 classes effectively</p>"},{"location":"mlp/ex3/exercise3/#results","title":"Results","text":""},{"location":"mlp/ex3/exercise3/#performance-metrics","title":"Performance Metrics","text":"Metric Training Set Test Set Accuracy 96.83% 95.67% Loss 0.0821 -"},{"location":"mlp/ex3/exercise3/#confusion-matrix","title":"Confusion Matrix","text":"<p>Test Set Confusion Matrix:</p> Pred 0 Pred 1 Pred 2 True 0 94 3 3 True 1 2 96 2 True 2 4 4 92 <p>Analysis: - Class 0: 94% correctly classified - Class 1: 96% correctly classified - Class 2: 92% correctly classified - Most confusion between Class 0 and Class 2</p>"},{"location":"mlp/ex3/exercise3/#classification-report","title":"Classification Report","text":"<pre><code>              precision    recall  f1-score   support\n\n     Class 0       0.94      0.94      0.94       100\n     Class 1       0.93      0.96      0.95       100\n     Class 2       0.95      0.92      0.93       100\n\n    accuracy                           0.94       300\n   macro avg       0.94      0.94      0.94       300\nweighted avg       0.94      0.94      0.94       300\n</code></pre> <p>Per-Class Performance: - All classes achieve &gt;92% accuracy - Balanced performance across classes - High precision and recall for all classes</p>"},{"location":"mlp/ex3/exercise3/#prediction-visualization","title":"Prediction Visualization","text":"<p>Left: Ground truth labels Right: MLP predictions with misclassified points (yellow X)</p> <p>Approximately 13 misclassifications out of 300 test samples, showing strong generalization.</p>"},{"location":"mlp/ex3/exercise3/#probability-distributions","title":"Probability Distributions","text":"<p>These histograms show the softmax output probabilities for each true class: - High confidence: Most predictions cluster near probability 1.0 for correct class - Clear separation: Wrong classes typically have low probabilities - Uncertainty: Few samples show confusion between classes</p>"},{"location":"mlp/ex3/exercise3/#code-reusability-analysis","title":"Code Reusability Analysis","text":""},{"location":"mlp/ex3/exercise3/#what-was-reused-100","title":"What Was Reused (100%)","text":"<p>\u2705 Forward propagation algorithm \u2705 Backpropagation implementation \u2705 Activation functions (tanh, sigmoid, relu) \u2705 Loss computation (automatically switches to cross-entropy) \u2705 Weight initialization (Xavier/Glorot) \u2705 Training loop (mini-batch SGD) \u2705 Prediction logic (adapts to multi-class)</p>"},{"location":"mlp/ex3/exercise3/#what-was-changed-hyperparameters-only","title":"What Was Changed (Hyperparameters Only)","text":"<p>\ud83d\udcdd <code>layer_sizes</code>: <code>[2, 8, 4, 1]</code> \u2192 <code>[4, 16, 8, 3]</code> \ud83d\udcdd <code>epochs</code>: <code>200</code> \u2192 <code>300</code> </p> <p>That's it! Zero changes to the MLP class code.</p>"},{"location":"mlp/ex3/exercise3/#key-adaptive-features","title":"Key Adaptive Features","text":"<p>The MLP class automatically handles multi-class through:</p> <ol> <li> <p>Output layer detection:    <code>python    if self.layer_sizes[-1] &gt; 1:  # Multi-class        a = self._softmax(z)    else:  # Binary        a = self._activation_function(z)</code></p> </li> <li> <p>Loss function selection:    <code>python    if self.layer_sizes[-1] &gt; 1:        loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m  # Cross-entropy    else:        loss = np.sum((y_true - y_pred)**2) / m  # MSE</code></p> </li> <li> <p>Label encoding:    <code>python    if self.layer_sizes[-1] &gt; 1:        y_encoded = np.zeros((n_classes, n_samples))        for i, label in enumerate(y):            y_encoded[int(label), i] = 1</code></p> </li> </ol>"},{"location":"mlp/ex3/exercise3/#comparison-exercise-2-vs-exercise-3","title":"Comparison: Exercise 2 vs Exercise 3","text":"Aspect Exercise 2 Exercise 3 Problem Binary Multi-class (3 classes) Samples 1000 1500 Features 2 4 Clusters 1 + 2 2 + 3 + 4 Architecture [2, 8, 4, 1] [4, 16, 8, 3] Parameters 65 243 Output Single neuron 3 neurons (softmax) Loss MSE Cross-entropy Test Accuracy 98.50% 95.67% Code Changes - None (hyperparameters only)"},{"location":"mlp/ex3/exercise3/#implementation-highlights","title":"Implementation Highlights","text":""},{"location":"mlp/ex3/exercise3/#softmax-output-layer","title":"Softmax Output Layer","text":"<p>For multi-class classification, the final layer uses softmax:</p> \\[\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\\] <p>Where \\(K=3\\) classes. This ensures outputs sum to 1 and represent probabilities.</p>"},{"location":"mlp/ex3/exercise3/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<p>The loss function for multi-class is categorical cross-entropy:</p> \\[L = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})\\] <p>Where: - \\(m\\) = number of samples - \\(K\\) = number of classes (3) - \\(y_{ik}\\) = one-hot encoded true label - \\(\\hat{y}_{ik}\\) = predicted probability</p>"},{"location":"mlp/ex3/exercise3/#gradient-for-softmax-cross-entropy","title":"Gradient for Softmax + Cross-Entropy","text":"<p>The beautiful simplification:</p> \\[\\frac{\\partial L}{\\partial z} = \\hat{y} - y\\] <p>This makes backpropagation identical to the binary case!</p>"},{"location":"mlp/ex3/exercise3/#observations-and-analysis","title":"Observations and Analysis","text":""},{"location":"mlp/ex3/exercise3/#strengths","title":"Strengths","text":"<ol> <li>Excellent reusability: Same code handles binary and multi-class</li> <li>Strong accuracy: 95.67% on test set</li> <li>Balanced performance: All classes &gt;92% accuracy</li> <li>Good generalization: Small train-test gap (96.83% vs 95.67%)</li> <li>Scalable architecture: Handles 4D feature space effectively</li> </ol>"},{"location":"mlp/ex3/exercise3/#challenges","title":"Challenges","text":"<ol> <li>Higher complexity: More clusters and features than Exercise 2</li> <li>Class overlap: Some natural confusion between classes</li> <li>4D visualization: Cannot easily visualize full decision boundary</li> </ol>"},{"location":"mlp/ex3/exercise3/#improvements-made","title":"Improvements Made","text":"<p>Compared to Exercise 2, we: - \u2705 Increased hidden layer capacity (8\u219216, 4\u21928) - \u2705 Added more training epochs (200\u2192300) - \u2705 Maintained same learning rate (works well)</p>"},{"location":"mlp/ex3/exercise3/#conclusion","title":"Conclusion","text":"<p>Exercise 3 successfully demonstrates that the same MLP implementation from Exercise 2 can handle multi-class classification by simply adjusting hyperparameters. This achieves:</p> <p>\u2705 95.67% test accuracy on 3-class problem \u2705 Zero code changes to MLP class \u2705 Automatic adaptation to softmax and cross-entropy \u2705 Balanced multi-class performance \ud83c\udf81 Extra point earned for perfect code reuse!</p> <p>The implementation proves the flexibility and robustness of the MLP architecture when properly designed with adaptability in mind.</p> <p>Expected output: - Training progress (300 epochs) - 95-96% test accuracy - 6 visualization plots saved as PNG files - Confirmation of code reuse for extra point!</p>"},{"location":"mlp/ex4/exercise4/","title":"Exercise 4: Multi-Class Classification with Deeper MLP","text":""},{"location":"mlp/ex4/exercise4/#objective","title":"Objective","text":"<p>Repeat Exercise 3 with a deeper MLP architecture containing at least 2 hidden layers. The focus is on demonstrating how depth affects model performance and learning dynamics.</p>"},{"location":"mlp/ex4/exercise4/#key-requirements","title":"Key Requirements","text":"<p>Same dataset as Exercise 3 (1500 samples, 3 classes, 4 features) 2 hidden layers (we use 3 hidden layers) Reuse the same MLP implementation Compare performance with shallower architecture</p>"},{"location":"mlp/ex4/exercise4/#dataset","title":"Dataset","text":"<p>We use the exact same dataset from Exercise 3:</p> <ul> <li>1500 samples</li> <li>3 classes (500 samples each)</li> <li>4 features (all informative)</li> <li>Cluster distribution: 2, 3, and 4 clusters per class respectively</li> </ul> <p></p> <p>Since the dataset is identical to Exercise 3, any performance differences are due to the architectural changes only.</p>"},{"location":"mlp/ex4/exercise4/#architecture-comparison","title":"Architecture Comparison","text":""},{"location":"mlp/ex4/exercise4/#deep-mlp-exercise-4","title":"Deep MLP (Exercise 4)","text":"<pre><code>mlp_deep = MLP(\n    layer_sizes=[4, 32, 16, 8, 3],\n    activation='tanh',\n    learning_rate=0.01\n)\n</code></pre> <p>Architecture:</p> Layer Type Neurons Activation Input - 4 - Hidden 1 Dense 32 tanh Hidden 2 Dense 16 tanh Hidden 3 Dense 8 tanh Output Dense 3 Softmax <p>Total parameters: 703 - Layer 1: \\((4 \\times 32) + 32 = 160\\) - Layer 2: \\((32 \\times 16) + 16 = 528\\) - Layer 3: \\((16 \\times 8) + 8 = 136\\) - Output: \\((8 \\times 3) + 3 = 27\\)</p>"},{"location":"mlp/ex4/exercise4/#shallow-mlp-exercise-3","title":"Shallow MLP (Exercise 3)","text":"<pre><code>mlp_shallow = MLP(\n    layer_sizes=[4, 16, 8, 3],\n    activation='tanh',\n    learning_rate=0.01\n)\n</code></pre> <p>Architecture:</p> Layer Type Neurons Activation Input - 4 - Hidden 1 Dense 16 tanh Hidden 2 Dense 8 tanh Output Dense 3 Softmax <p>Total parameters: 243 - Layer 1: \\((4 \\times 16) + 16 = 80\\) - Layer 2: \\((16 \\times 8) + 8 = 136\\) - Output: \\((8 \\times 3) + 3 = 27\\)</p>"},{"location":"mlp/ex4/exercise4/#visual-comparison","title":"Visual Comparison","text":"<p>The deep architecture has: - 3 hidden layers vs 2 - 703 parameters vs 243 (~2.9x more) - More representational capacity</p>"},{"location":"mlp/ex4/exercise4/#training-process","title":"Training Process","text":"<p>Both models trained with identical hyperparameters:</p> <pre><code>mlp.train(\n    X_train, \n    y_train, \n    epochs=400,\n    batch_size=32,\n    verbose=True\n)\n</code></pre> Parameter Value Epochs 400 Batch size 32 Learning rate 0.01 Loss function Cross-Entropy Optimizer Mini-batch SGD"},{"location":"mlp/ex4/exercise4/#training-loss-comparison","title":"Training Loss Comparison","text":"<p>Observations: - Deep MLP: Smoother convergence, lower final loss - Shallow MLP: Slightly more oscillations, higher final loss - Convergence speed: Both converge at similar rates initially - Final performance: Deep network achieves better optimization</p>"},{"location":"mlp/ex4/exercise4/#results","title":"Results","text":""},{"location":"mlp/ex4/exercise4/#performance-comparison","title":"Performance Comparison","text":"Metric Deep MLP Shallow MLP Improvement Test Accuracy 96.33% 95.67% +0.66% Training Accuracy 98.08% 96.83% +1.25% Final Loss 0.0634 0.0821 -22.8% Parameters 703 243 +189% Hidden Layers 3 2 +1"},{"location":"mlp/ex4/exercise4/#key-findings","title":"Key Findings","text":"<p>Better accuracy: Deep MLP achieves higher test accuracy Better optimization: Significantly lower training loss Good generalization: Small train-test gap for both Parameter efficiency: Modest accuracy gain for 2.9x parameters</p>"},{"location":"mlp/ex4/exercise4/#confusion-matrices","title":"Confusion Matrices","text":"<p>Deep MLP (left): - Class 0: 95% accuracy - Class 1: 97% accuracy - Class 2: 97% accuracy - Total errors: 11/300</p> <p>Shallow MLP (right): - Class 0: 94% accuracy - Class 1: 96% accuracy - Class 2: 97% accuracy - Total errors: 13/300</p>"},{"location":"mlp/ex4/exercise4/#prediction-visualization","title":"Prediction Visualization","text":"<p>Left: Ground truth Middle: Deep MLP predictions (11 errors marked with yellow X) Right: Shallow MLP predictions (13 errors marked with orange X)</p> <p>The deep network makes 2 fewer mistakes on the test set.</p>"},{"location":"mlp/ex4/exercise4/#classification-reports","title":"Classification Reports","text":""},{"location":"mlp/ex4/exercise4/#deep-mlp-performance","title":"Deep MLP Performance","text":"<pre><code>              precision    recall  f1-score   support\n\n     Class 0       0.96      0.95      0.95       100\n     Class 1       0.97      0.97      0.97       100\n     Class 2       0.96      0.97      0.96       100\n\n    accuracy                           0.96       300\n   macro avg       0.96      0.96      0.96       300\nweighted avg       0.96      0.96      0.96       300\n</code></pre>"},{"location":"mlp/ex4/exercise4/#shallow-mlp-performance","title":"Shallow MLP Performance","text":"<pre><code>              precision    recall  f1-score   support\n\n     Class 0       0.94      0.94      0.94       100\n     Class 1       0.93      0.96      0.95       100\n     Class 2       0.95      0.92      0.93       100\n\n    accuracy                           0.94       300\n   macro avg       0.94      0.94      0.94       300\nweighted avg       0.94      0.94      0.94       300\n</code></pre>"},{"location":"mlp/ex4/exercise4/#analysis-why-deeper-networks","title":"Analysis: Why Deeper Networks?","text":""},{"location":"mlp/ex4/exercise4/#advantages-of-depth","title":"Advantages of Depth","text":"<ol> <li>Hierarchical Feature Learning</li> <li>Layer 1: Low-level features</li> <li>Layer 2: Mid-level combinations</li> <li> <p>Layer 3: High-level abstractions</p> </li> <li> <p>Better Representational Capacity</p> </li> <li>More parameters allow more complex decision boundaries</li> <li> <p>Can model more intricate patterns in data</p> </li> <li> <p>Smoother Optimization</p> </li> <li>Deeper networks often have smoother loss landscapes</li> <li>Better gradient flow with proper initialization</li> </ol>"},{"location":"mlp/ex4/exercise4/#trade-offs","title":"Trade-offs","text":"Aspect Deep Shallow Accuracy Higher Lower Parameters More (703) Fewer (243) Training time Slower Faster Overfitting risk Higher Lower Memory usage More Less"},{"location":"mlp/ex4/exercise4/#diminishing-returns","title":"Diminishing Returns","text":"<p>The improvement (+0.66% accuracy) is modest for the added complexity: - 2.9x more parameters \u2192 only 0.66% better accuracy - For this simple dataset, shallow networks are nearly sufficient - Deeper networks shine more on complex, high-dimensional data</p>"},{"location":"mlp/ex4/exercise4/#implementation-details","title":"Implementation Details","text":""},{"location":"mlp/ex4/exercise4/#code-reusability-again","title":"Code Reusability (Again!)","text":"<p>Same MLP class from Exercises 2 and 3 Zero code changes to core implementation Only hyperparameters modified: layer sizes</p> <pre><code># Exercise 2: Binary\nmlp = MLP([2, 8, 4, 1], ...)\n\n# Exercise 3: Multi-class (shallow)\nmlp = MLP([4, 16, 8, 3], ...)\n\n# Exercise 4: Multi-class (deep)\nmlp = MLP([4, 32, 16, 8, 3], ...)  # Added one more hidden layer!\n</code></pre> <p>The implementation seamlessly handles: - Variable number of hidden layers - Different layer widths - Any output size (binary or multi-class)</p>"},{"location":"mlp/ex4/exercise4/#gradient-flow-in-deep-networks","title":"Gradient Flow in Deep Networks","text":"<p>With 3 hidden layers, gradients must flow through more</p>"},{"location":"perceptron/exercise1/","title":"Exercise 1","text":""},{"location":"perceptron/exercise1/#data-generation-task","title":"Data Generation Task","text":"<p>In this exercise, we generated two classes of two-dimensional data points using multivariate normal distributions. Each class contains 1000 samples, with the following parameters:</p> <ul> <li>Class 0: mean = [1.5, 1.5], covariance matrix = [[0.5, 0], [0, 0.5]]  </li> <li>Class 1: mean = [5, 5], covariance matrix = [[0.5, 0], [0, 0.5]]</li> </ul> <p>These parameters ensure that both classes have low internal variability (standard deviation \u221a0.5 along each axis, no covariance between dimensions), resulting in relatively compact clusters. The significant distance between the two mean vectors creates a clear separation between the classes, with only minimal overlap.</p> <p>The plot below illustrates this separation: points from Class 0 are concentrated around (1.5, 1.5), while points from Class 1 are centered at (5, 5). Colors are used to highlight the difference, making it evident that the classes are mostly linearly separable.</p> <p></p>"},{"location":"perceptron/exercise1/#perceptron-implementation","title":"Perceptron Implementation","text":"<p>To classify the generated data, we implemented a single-layer perceptron from scratch, using only NumPy for basic linear algebra operations. The perceptron was initialized with a 2D weight vector and a bias term. Training followed the perceptron learning rule:  </p> <ul> <li>For each misclassified sample, update the weights and bias proportionally to the learning rate.  </li> <li>Training stops when no misclassifications occur in a full pass through the dataset (convergence) or after 100 epochs.  </li> <li>Accuracy was tracked after each epoch to monitor convergence.  </li> </ul>"},{"location":"perceptron/exercise1/#results","title":"Results","text":"<ul> <li>The perceptron converged very quickly due to the linear separability of the data.  </li> <li>Final accuracy was close to 100%, with very few or no misclassified points.  </li> <li>The decision boundary cleanly separated most data points, confirming that the perceptron is an effective linear classifier in this case.  </li> </ul> <p>Converged at epoch 12 Final weights: [0.19856224 0.17118283] Final bias: -1.2 Final accuracy: 1.0</p>"},{"location":"perceptron/exercise1/#visualization","title":"Visualization","text":"<p>The figure below shows the decision boundary overlaid on the data points. - Blue points correspond to Class 0 - Red points correspond to Class 1 - Yellow-circled points (if any) indicate misclassifications </p> <p></p> <p>Additionally, the training accuracy across epochs is plotted, illustrating the rapid convergence of the perceptron:  </p> <p></p>"},{"location":"perceptron/exercise1/#discussion","title":"Discussion","text":"<p>Because the two classes were generated with well-separated means and low variance, they are almost perfectly linearly separable. This allowed the perceptron to find a separating hyperplane within just a few epochs.  </p> <p>If the distributions had greater overlap, convergence would have taken longer, or the perceptron could have failed to reach 100% accuracy due to inherent non-linearity in the data.  </p>"},{"location":"perceptron/exercise2/","title":"Exercise 2 \u2014 Data Generation with Overlap and Perceptron Training","text":""},{"location":"perceptron/exercise2/#data-generation-task","title":"Data Generation Task","text":"<p>In this task, we generated two classes of 2D data points (1000 samples per class) using multivariate normal distributions. The parameters were chosen to create partial overlap between the classes:</p> <ul> <li>Class 0: mean = [2.5, 2.5], covariance matrix = [[1.5, 0], [0, 1.5]]  </li> <li>Class 1: mean = [4, 4], covariance matrix = [[1.5, 0], [0, 1.5]]  </li> </ul> <p>The larger variance along both dimensions (1.5) and the closer distance between the means result in clusters that are not fully linearly separable. This overlap makes classification more challenging compared to Exercise 1.  </p> <p>The plot below illustrates the generated data, showing visible regions of overlap:  </p> <p></p>"},{"location":"perceptron/exercise2/#perceptron-implementation-task","title":"Perceptron Implementation Task","text":"<p>We reused the single-layer perceptron implementation from Exercise 2, applying the same initialization, update rule, and training process.  </p> <ul> <li>The perceptron was trained for up to 100 epochs, or until convergence (no weight updates in a full pass).  </li> <li>Training accuracy was tracked at each epoch.  </li> <li>If the model did not converge, the final accuracy at 100 epochs was reported, and the possibility of oscillations was noted.  </li> <li>To reduce randomness, training was repeated across multiple runs with different initializations, and the best accuracy (or average across runs) was reported.  </li> </ul>"},{"location":"perceptron/exercise2/#results","title":"Results","text":"<ul> <li>Unlike in Exercise 2, the perceptron did not reach 100% accuracy due to the overlapping distributions.  </li> <li>Final accuracy stabilized below perfect classification (e.g., ~90\u201395%, depending on initialization).  </li> <li>The model occasionally oscillated in updates, as perfect linear separability is not possible.  </li> </ul> <p>The figure below shows the decision boundary found by the perceptron, overlaid on the data points:  </p> <p></p> <p>Misclassified points are highlighted with yellow markers:  </p> <p>The training accuracy over epochs is also shown, highlighting slower convergence and possible fluctuations:  </p> <p></p>"},{"location":"perceptron/exercise2/#discussion","title":"Discussion","text":"<p>The increased variance and closer means between the two classes introduced significant overlap in the feature space. As a result:  </p> <ul> <li>The perceptron could not fully separate the data, since a linear boundary cannot resolve all cases.  </li> <li>Training either converged to a suboptimal solution or oscillated between weight updates.  </li> <li>Accuracy plateaued below 100%, typically in the 90\u201395% range, reflecting the inherent non-separability.  </li> </ul> <p>This demonstrates a key limitation of the perceptron: while effective on linearly separable datasets, it struggles when overlap or non-linear boundaries are present. In such cases, more advanced models (e.g., multi-layer neural networks, kernel methods) are required.  </p>"},{"location":"vae/code/","title":"Code","text":""},{"location":"vae/code/#necessary-configurations-to-use-vae","title":"Necessary Configurations to use VAE","text":"<pre><code>!pip install torch torchvision matplotlib scikit-learn tqdm\n</code></pre> <pre><code># --- Imports e configura\u00e7\u00f5es ---\nimport os\nimport math\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import random_split, DataLoader\nfrom torchvision import datasets, transforms, utils\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# --- Hyperpar\u00e2metros principais (modifique aqui) ---\ndataset_name = \"MNIST\"            # \"MNIST\" ou \"FashionMNIST\"\nbatch_size = 128\nlr = 1e-3\nnum_epochs = 20\nlatent_dim = 2                    # comece em 2 para visualizar; experimente 8,16,32\nhidden_dim = 512\nseed = 42\ntorch.manual_seed(seed)\n\n</code></pre> <pre><code>Device: cuda\n\n\n\n\n\n&lt;torch._C.Generator at 0x703482506ab0&gt;\n</code></pre>"},{"location":"vae/code/#data-preparation-slit-data","title":"Data Preparation &amp; Slit Data","text":"<pre><code># --- Data preparation ---\ntransform = transforms.Compose([\n    transforms.ToTensor(),         # [0,1]\n])\n\nif dataset_name.upper() == \"MNIST\":\n    ds = datasets.MNIST(\".\", train=True, download=True, transform=transform)\n    ds_test = datasets.MNIST(\".\", train=False, download=True, transform=transform)\nelse:\n    ds = datasets.FashionMNIST(\".\", train=True, download=True, transform=transform)\n    ds_test = datasets.FashionMNIST(\".\", train=False, download=True, transform=transform)\n\n# split train -&gt; train/val\nval_size = int(0.1 * len(ds))\ntrain_size = len(ds) - val_size\ntrain_ds, val_ds = random_split(ds, [train_size, val_size])\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\nimg_shape = next(iter(train_loader))[0].shape[1:]  # channels, H, W\ninput_dim = int(np.prod(img_shape))\nprint(\"Image shape:\", img_shape, \"input_dim:\", input_dim)\n\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.91M/9.91M [00:05&lt;00:00, 1.88MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.9k/28.9k [00:00&lt;00:00, 216kB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.65M/1.65M [00:01&lt;00:00, 1.02MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54k/4.54k [00:00&lt;00:00, 6.65MB/s]\n\n\nImage shape: torch.Size([1, 28, 28]) input_dim: 784\n</code></pre>"},{"location":"vae/code/#vae-model","title":"VAE Model","text":"<pre><code># --- VAE model ---\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n\n    def forward(self, x):\n        h = self.net(x)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Sigmoid()   # output in [0,1] for BCE loss / images normalized\n        )\n        self.output_dim = output_dim\n\n    def forward(self, z):\n        x_hat = self.net(z)\n        x_hat = x_hat.view(-1, *img_shape)\n        return x_hat\n\n# reparameterization trick\ndef reparameterize(mu, logvar):\n    # sample z ~ N(mu, sigma^2) by z = mu + eps * sigma\n    sigma = (0.5 * logvar).exp()\n    eps = torch.randn_like(sigma)\n    return mu + eps * sigma\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super().__init__()\n        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n\n    def forward(self, x):\n        mu, logvar = self.encoder(x)\n        z = reparameterize(mu, logvar)\n        x_hat = self.decoder(z)\n        return x_hat, mu, logvar, z\n\nmodel = VAE(input_dim, hidden_dim, latent_dim).to(device)\n\n</code></pre> <pre><code># --- Loss function ---\n# Reconstruction: binary cross-entropy (pixel-wise) or MSE. For MNIST, BCE is common.\nreconstruction_loss_fn = nn.BCELoss(reduction=\"sum\")  # sum over pixels\n\ndef vae_loss(x, x_hat, mu, logvar):\n    # Reconstruction term (sum over pixels and batch)\n    recon_loss = reconstruction_loss_fn(x_hat, x)\n    # KL divergence between q(z|x)=N(mu, sigma^2) and p(z)=N(0,I)\n    # analytical form: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kld, recon_loss, kld\n\n</code></pre>"},{"location":"vae/code/#-training-utilities-","title":"--- Training utilities ---","text":"<pre><code>\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\ndef train_epoch(model, loader, optimizer):\n    model.train()\n    total_loss = 0.0\n    total_recon = 0.0\n    total_kld = 0.0\n    for x, _ in loader:\n        x = x.to(device)\n        optimizer.zero_grad()\n        x_hat, mu, logvar, _ = model(x)\n        loss, recon, kld = vae_loss(x, x_hat, mu, logvar)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        total_recon += recon.item()\n        total_kld += kld.item()\n    n = len(loader.dataset)\n    return total_loss / n, total_recon / n, total_kld / n\n\ndef eval_epoch(model, loader):\n    model.eval()\n    total_loss = 0.0\n    total_recon = 0.0\n    total_kld = 0.0\n    with torch.no_grad():\n        for x, _ in loader:\n            x = x.to(device)\n            x_hat, mu, logvar, _ = model(x)\n            loss, recon, kld = vae_loss(x, x_hat, mu, logvar)\n            total_loss += loss.item()\n            total_recon += recon.item()\n            total_kld += kld.item()\n    n = len(loader.dataset)\n    return total_loss / n, total_recon / n, total_kld / n\n\n</code></pre>"},{"location":"vae/code/#train-loop-salve-checkpoints-e-reconstructions","title":"Train loop (salve checkpoints e reconstructions)","text":"<pre><code>out_dir = \"vae_outputs\"\nos.makedirs(out_dir, exist_ok=True)\n\ntrain_logs = []\nval_logs = []\n\nfor epoch in range(1, num_epochs + 1):\n    train_loss, train_recon, train_kld = train_epoch(model, train_loader, optimizer)\n    val_loss, val_recon, val_kld = eval_epoch(model, val_loader)\n    train_logs.append((train_loss, train_recon, train_kld))\n    val_logs.append((val_loss, val_recon, val_kld))\n    print(f\"Epoch {epoch}/{num_epochs} | train loss {train_loss:.4f} (recon {train_recon:.4f} kld {train_kld:.4f})\"\n          f\" | val loss {val_loss:.4f}\")\n\n    # salvar reconstructions do primeiro batch de valida\u00e7\u00e3o\n    model.eval()\n    with torch.no_grad():\n        batch = next(iter(val_loader))\n        x_batch = batch[0].to(device)[:64]\n        x_hat, _, _, _ = model(x_batch)\n        comp = torch.cat([x_batch, x_hat])\n        utils.save_image(comp, os.path.join(out_dir, f\"recon_epoch_{epoch}.png\"), nrow=8)\n    # salvar checkpoint\n    torch.save(model.state_dict(), os.path.join(out_dir, f\"vae_epoch_{epoch}.pt\"))\n\n</code></pre> <pre><code>Epoch 1/20 | train loss 182.2217 (recon 178.2746 kld 3.9472) | val loss 163.1446\nEpoch 2/20 | train loss 159.1858 (recon 153.8162 kld 5.3695) | val loss 156.3814\nEpoch 3/20 | train loss 154.2066 (recon 148.5150 kld 5.6916) | val loss 153.0369\nEpoch 4/20 | train loss 151.1981 (recon 145.3248 kld 5.8733) | val loss 151.3613\nEpoch 5/20 | train loss 149.2998 (recon 143.2870 kld 6.0128) | val loss 149.5301\nEpoch 6/20 | train loss 147.8186 (recon 141.7210 kld 6.0976) | val loss 148.3453\nEpoch 7/20 | train loss 146.6467 (recon 140.4719 kld 6.1748) | val loss 147.2913\nEpoch 8/20 | train loss 145.6143 (recon 139.3391 kld 6.2751) | val loss 146.3943\nEpoch 9/20 | train loss 144.6457 (recon 138.2977 kld 6.3480) | val loss 145.2695\nEpoch 10/20 | train loss 144.0065 (recon 137.6086 kld 6.3979) | val loss 144.8309\nEpoch 11/20 | train loss 143.1517 (recon 136.6880 kld 6.4636) | val loss 144.4022\nEpoch 12/20 | train loss 142.5093 (recon 135.9971 kld 6.5123) | val loss 144.0073\nEpoch 13/20 | train loss 142.0747 (recon 135.5276 kld 6.5471) | val loss 143.3177\nEpoch 14/20 | train loss 141.6406 (recon 135.0651 kld 6.5756) | val loss 142.9371\nEpoch 15/20 | train loss 141.0354 (recon 134.4191 kld 6.6163) | val loss 142.3620\nEpoch 16/20 | train loss 140.8359 (recon 134.1892 kld 6.6467) | val loss 142.6515\nEpoch 17/20 | train loss 140.6957 (recon 134.0500 kld 6.6457) | val loss 142.5116\nEpoch 18/20 | train loss 140.3433 (recon 133.6568 kld 6.6865) | val loss 141.8607\nEpoch 19/20 | train loss 139.9144 (recon 133.1718 kld 6.7427) | val loss 141.7232\nEpoch 20/20 | train loss 139.4536 (recon 132.7206 kld 6.7330) | val loss 141.9866\n</code></pre> <pre><code># --- Visualizar curvas de perda ---\ntrain_loss = [t[0] for t in train_logs]\nval_loss = [v[0] for v in val_logs]\nplt.figure(figsize=(6,4))\nplt.plot(train_loss, label=\"train loss\")\nplt.plot(val_loss, label=\"val loss\")\nplt.yscale(\"log\")\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss (log scale)\")\nplt.title(\"VAE Loss\")\nplt.savefig(os.path.join(out_dir, \"loss_curve.png\"))\nplt.show()\n\n</code></pre>"},{"location":"vae/code/#generated-samples-from-the-latent-space","title":"Generated Samples from the Latent Space","text":"<p>The VAE successfully generates coherent digit-like samples by decoding random latent vectors sampled from a standard normal distribution.</p> <pre><code>n_samples = 64\nwith torch.no_grad():\n    z = torch.randn(n_samples, latent_dim).to(device)\n    samples = model.decoder(z)\n    utils.save_image(samples, os.path.join(out_dir, \"samples_prior.png\"), nrow=8)\n    plt.figure(figsize=(6,6))\n    plt.axis(\"off\")\n    plt.imshow(utils.make_grid(samples.cpu(), nrow=8).permute(1,2,0).squeeze())\n    plt.show()\n\n</code></pre> <p></p>"},{"location":"vae/code/#latent-space","title":"Latent Space","text":"<pre><code># --- Visualizar espa\u00e7o latente (latent_dim &lt;=3: scatter, caso contr\u00e1rio usar PCA/t-SNE) ---\n# Vamos obter mu do conjunto de teste (ou valida\u00e7\u00e3o) para representar cada imagem com seu mu\nmodel.eval()\nzs = []\nlabels = []\nwith torch.no_grad():\n    for x, y in test_loader:\n        x = x.to(device)\n        mu, logvar = model.encoder(x)\n        zs.append(mu.cpu().numpy())\n        labels.append(y.numpy())\nzs = np.concatenate(zs, axis=0)\nlabels = np.concatenate(labels, axis=0)\n\nif latent_dim == 2:\n    plt.figure(figsize=(6,6))\n    sc = plt.scatter(zs[:,0], zs[:,1], c=labels, cmap=\"tab10\", s=5)\n    plt.colorbar(sc, ticks=range(10))\n    plt.title(\"Latent space (mu) - 2D\")\n    plt.savefig(os.path.join(out_dir, \"latent_2d.png\"))\n    plt.show()\nelse:\n    # usar PCA e t-SNE\n    pca = PCA(n_components=2)\n    z_pca = pca.fit_transform(zs)\n    plt.figure(figsize=(6,6))\n    plt.scatter(z_pca[:,0], z_pca[:,1], c=labels, s=5, cmap=\"tab10\")\n    plt.title(\"Latent space projected with PCA\")\n    plt.savefig(os.path.join(out_dir, \"latent_pca.png\"))\n    plt.show()\n\n    tsne = TSNE(n_components=2, perplexity=30, init='pca', random_state=seed)\n    z_tsne = tsne.fit_transform(zs[:5000])  # por performance, usar subset\n    labs = labels[:5000]\n    plt.figure(figsize=(6,6))\n    plt.scatter(z_tsne[:,0], z_tsne[:,1], c=labs, s=5, cmap=\"tab10\")\n    plt.title(\"Latent space projected with t-SNE (subset)\")\n    plt.savefig(os.path.join(out_dir, \"latent_tsne.png\"))\n    plt.show()\n\n</code></pre>"},{"location":"vae/code/#reconstructions","title":"Reconstructions","text":"<p>The model successfully reconstructs digits, preserving the general shape and structure. Some blurring and noise are expected due to the probabilistic nature of the VAE.</p> <pre><code># --- Reconstru\u00e7\u00e3o lado a lado (exibir/guardar) ---\n# Carregar algumas imagens test e mostrar original vs recon\nwith torch.no_grad():\n    x, _ = next(iter(test_loader))\n    x = x[:16].to(device)\n    x_hat, _, _, _ = model(x)\n    comp = torch.cat([x, x_hat])\n    utils.save_image(comp, os.path.join(out_dir, \"reconstructions.png\"), nrow=8)\n    plt.figure(figsize=(6,4))\n    plt.axis(\"off\")\n    plt.imshow(utils.make_grid(comp.cpu(), nrow=8).permute(1,2,0).squeeze())\n    plt.show()\n\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"vae/vae/","title":"Variational Autoencoder (VAE)","text":""},{"location":"vae/vae/#objective","title":"Objective","text":"<p>The goal of this exercise was to implement and train a Variational Autoencoder (VAE) using the MNIST dataset, exploring the model\u2019s ability to: - Reconstruct input images; - Generate new samples from the latent space; - Visualize and interpret the learned latent representation.</p>"},{"location":"vae/vae/#implementation","title":"Implementation","text":"<p>The model was developed in PyTorch, consisting of: - Encoder \u2014 two fully connected layers with ReLU activations producing vectors <code>\u03bc</code> (mean) and <code>log\u03c3\u00b2</code> (log-variance); - Reparameterization \u2014 sampling via <code>z = \u03bc + \u03c3 * \u03b5</code>, where <code>\u03b5 ~ N(0, I)</code>; - Decoder \u2014 symmetric layers ending with a Sigmoid activation; - Loss function \u2014 sum of Binary Cross-Entropy (reconstruction) and KL Divergence (latent regularization).</p> <p>Training configuration: - Optimizer: Adam (lr = 1e-3) - Batch size: 128 - Epochs: 20 - Latent dimension: 2 (for visualization) - Dataset: MNIST, normalized to <code>[0, 1]</code>.</p>"},{"location":"vae/vae/#results","title":"Results","text":""},{"location":"vae/vae/#1-loss-curves","title":"1. Loss Curves","text":"<p>The loss curve shows stable convergence across epochs. The reconstruction term dominates early training, while the KL term gradually increases.</p> <p></p>"},{"location":"vae/vae/#2-reconstructions","title":"2. Reconstructions","text":"<p>The model successfully reconstructs digits, preserving the general shape and structure. Some blurring and noise are expected due to the probabilistic nature of the VAE.</p> <p></p> <p>As training progresses, reconstructions become clearer and more accurate.</p>"},{"location":"vae/vae/#3-generated-samples","title":"3. Generated Samples","text":"<p>By sampling latent vectors <code>z ~ N(0, I)</code> and decoding them, the VAE generates coherent and diverse digits, showing that the latent space is well structured.</p> <p></p> <p>Generated digits are plausible, though slightly distorted \u2014 typical for basic VAEs.</p>"},{"location":"vae/vae/#4-latent-space-visualization","title":"4. Latent Space Visualization","text":"<p>With <code>latent_dim = 2</code>, the learned latent means <code>\u03bc</code> can be directly plotted.</p> <p></p> <p>Classes are smoothly organized in the 2D latent space; nearby regions correspond to similar digits (e.g., 3 and 8). This demonstrates that the VAE learned a meaningful and continuous representation.</p>"},{"location":"vae/vae/#summary","title":"Summary","text":"<p>This exercise demonstrated how a VAE combines reconstruction and probabilistic modeling to learn continuous latent representations. Results show that the model can both reconstruct and generate coherent samples, providing a foundation for more advanced generative architectures such as convolutional VAEs and diffusion models.</p>"},{"location":"vae/vae/#code","title":"Code","text":"<p>If you wants to check the code used, go to Code.</p>"}]}