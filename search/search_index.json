{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#exercicios","title":"Exerc\u00edcios","text":"<p>Bem-vindo! Aqui esta os resultados dos exercicios feitos.</p> <ul> <li>Perceptron</li> </ul>"},{"location":"perceptron/exercise1/","title":"Exercise 1","text":""},{"location":"perceptron/exercise1/#exercise-1","title":"Exercise 1","text":""},{"location":"perceptron/exercise1/#data-generation-task","title":"Data Generation Task","text":"<p>In this exercise, we generated two classes of two-dimensional data points using multivariate normal distributions. Each class contains 1000 samples, with the following parameters:</p> <ul> <li>Class 0: mean = [1.5, 1.5], covariance matrix = [[0.5, 0], [0, 0.5]]  </li> <li>Class 1: mean = [5, 5], covariance matrix = [[0.5, 0], [0, 0.5]]</li> </ul> <p>These parameters ensure that both classes have low internal variability (standard deviation \u221a0.5 along each axis, no covariance between dimensions), resulting in relatively compact clusters. The significant distance between the two mean vectors creates a clear separation between the classes, with only minimal overlap.</p> <p>The plot below illustrates this separation: points from Class 0 are concentrated around (1.5, 1.5), while points from Class 1 are centered at (5, 5). Colors are used to highlight the difference, making it evident that the classes are mostly linearly separable.</p> <p></p>"},{"location":"perceptron/exercise1/#perceptron-implementation","title":"Perceptron Implementation","text":"<p>To classify the generated data, we implemented a single-layer perceptron from scratch, using only NumPy for basic linear algebra operations. The perceptron was initialized with a 2D weight vector and a bias term. Training followed the perceptron learning rule:  </p> <ul> <li>For each misclassified sample, update the weights and bias proportionally to the learning rate.  </li> <li>Training stops when no misclassifications occur in a full pass through the dataset (convergence) or after 100 epochs.  </li> <li>Accuracy was tracked after each epoch to monitor convergence.  </li> </ul>"},{"location":"perceptron/exercise1/#results","title":"Results","text":"<ul> <li>The perceptron converged very quickly due to the linear separability of the data.  </li> <li>Final accuracy was close to 100%, with very few or no misclassified points.  </li> <li>The decision boundary cleanly separated most data points, confirming that the perceptron is an effective linear classifier in this case.  </li> </ul> <p>Converged at epoch 12 Final weights: [0.19856224 0.17118283] Final bias: -1.2 Final accuracy: 1.0</p>"},{"location":"perceptron/exercise1/#visualization","title":"Visualization","text":"<p>The figure below shows the decision boundary overlaid on the data points. - Blue points correspond to Class 0 - Red points correspond to Class 1 - Yellow-circled points (if any) indicate misclassifications </p> <p></p> <p>Additionally, the training accuracy across epochs is plotted, illustrating the rapid convergence of the perceptron:  </p> <p></p>"},{"location":"perceptron/exercise1/#discussion","title":"Discussion","text":"<p>Because the two classes were generated with well-separated means and low variance, they are almost perfectly linearly separable. This allowed the perceptron to find a separating hyperplane within just a few epochs.  </p> <p>If the distributions had greater overlap, convergence would have taken longer, or the perceptron could have failed to reach 100% accuracy due to inherent non-linearity in the data.  </p>"},{"location":"perceptron/exercise2/","title":"Exercise 2","text":""},{"location":"perceptron/exercise2/#exercise-2-data-generation-with-overlap-and-perceptron-training","title":"Exercise 2 \u2014 Data Generation with Overlap and Perceptron Training","text":""},{"location":"perceptron/exercise2/#data-generation-task","title":"Data Generation Task","text":"<p>In this task, we generated two classes of 2D data points (1000 samples per class) using multivariate normal distributions. The parameters were chosen to create partial overlap between the classes:</p> <ul> <li>Class 0: mean = [2.5, 2.5], covariance matrix = [[1.5, 0], [0, 1.5]]  </li> <li>Class 1: mean = [4, 4], covariance matrix = [[1.5, 0], [0, 1.5]]  </li> </ul> <p>The larger variance along both dimensions (1.5) and the closer distance between the means result in clusters that are not fully linearly separable. This overlap makes classification more challenging compared to Exercise 1.  </p> <p>The plot below illustrates the generated data, showing visible regions of overlap:  </p> <p></p>"},{"location":"perceptron/exercise2/#perceptron-implementation-task","title":"Perceptron Implementation Task","text":"<p>We reused the single-layer perceptron implementation from Exercise 2, applying the same initialization, update rule, and training process.  </p> <ul> <li>The perceptron was trained for up to 100 epochs, or until convergence (no weight updates in a full pass).  </li> <li>Training accuracy was tracked at each epoch.  </li> <li>If the model did not converge, the final accuracy at 100 epochs was reported, and the possibility of oscillations was noted.  </li> <li>To reduce randomness, training was repeated across multiple runs with different initializations, and the best accuracy (or average across runs) was reported.  </li> </ul>"},{"location":"perceptron/exercise2/#results","title":"Results","text":"<ul> <li>Unlike in Exercise 2, the perceptron did not reach 100% accuracy due to the overlapping distributions.  </li> <li>Final accuracy stabilized below perfect classification (e.g., ~90\u201395%, depending on initialization).  </li> <li>The model occasionally oscillated in updates, as perfect linear separability is not possible.  </li> </ul> <p>The figure below shows the decision boundary found by the perceptron, overlaid on the data points:  </p> <p></p> <p>Misclassified points are highlighted with yellow markers:  </p> <p>The training accuracy over epochs is also shown, highlighting slower convergence and possible fluctuations:  </p> <p></p>"},{"location":"perceptron/exercise2/#discussion","title":"Discussion","text":"<p>The increased variance and closer means between the two classes introduced significant overlap in the feature space. As a result:  </p> <ul> <li>The perceptron could not fully separate the data, since a linear boundary cannot resolve all cases.  </li> <li>Training either converged to a suboptimal solution or oscillated between weight updates.  </li> <li>Accuracy plateaued below 100%, typically in the 90\u201395% range, reflecting the inherent non-separability.  </li> </ul> <p>This demonstrates a key limitation of the perceptron: while effective on linearly separable datasets, it struggles when overlap or non-linear boundaries are present. In such cases, more advanced models (e.g., multi-layer neural networks, kernel methods) are required.  </p>"}]}