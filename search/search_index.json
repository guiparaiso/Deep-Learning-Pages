{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Exerc\u00edcios","text":"<p>Bem-vindo! Aqui esta os resultados dos exercicios feitos.</p> <ul> <li>Data</li> <li>Perceptron</li> </ul>"},{"location":"data/ex1/exercise1/","title":"Exercise 1 \u2014 Exploring Class Separability in 2D","text":""},{"location":"data/ex1/exercise1/#1-dataset-generation","title":"1. Dataset Generation","text":"<p>A synthetic dataset was created with a total of 400 samples, equally divided among 4 classes (100 each). Each class was drawn from a 2D Gaussian distribution with the following parameters:</p> Class Mean (\u03bc) Standard Deviation (\u03c3) 0 [2, 3] [0.8, 2.5] 1 [5, 6] [1.2, 1.9] 2 [8, 1] [0.9, 0.9] 3 [15, 4] [0.5, 2.0] <p>The resulting data points form four distinct clusters in the 2D plane.</p>"},{"location":"data/ex1/exercise1/#2-scatter-plot-and-observations","title":"2. Scatter Plot and Observations","text":"<p>Below is the scatter plot showing the 400 samples colored by class:</p> <p></p> <p>Observations: - Class 0 is centered around (2, 3) and vertically elongated because of the large \u03c3_y = 2.5. - Class 1 (mean \u2248 (5, 6)) forms a more compact cluster slightly above Class 0. - Class 2 (mean \u2248 (8, 1)) lies in the lower-right area and is fairly circular. - Class 3 (mean \u2248 (15, 4)) is far to the right, clearly separated from the others.</p> <p>There is little overlap between most classes \u2014 only minor intersections between Classes 0 and 1 due to their proximity.</p>"},{"location":"data/ex1/exercise1/#3-linear-vs-non-linear-decision-boundaries","title":"3. Linear vs Non-linear Decision Boundaries","text":"<p>Two models were trained to visualize the decision regions:</p> <ul> <li>Logistic Regression (linear classifier)</li> <li>MLP with tanh activation (non-linear classifier)</li> </ul>"},{"location":"data/ex1/exercise1/#linear-model-logistic-regression","title":"Linear Model (Logistic Regression)","text":"<p>The linear model creates straight decision boundaries, effectively separating most clusters. Its test accuracy reached 0.98, showing that the dataset is almost linearly separable.</p>"},{"location":"data/ex1/exercise1/#non-linear-model-mlp-with-tanh","title":"Non-linear Model (MLP with tanh)","text":"<p>The MLP achieved perfect accuracy (1.00) and learned slightly curved boundaries, allowing a smoother fit between classes with small overlap.</p>"},{"location":"data/ex1/exercise1/#4-manual-boundary-sketch","title":"4. Manual Boundary Sketch","text":"<p>Below is a scatter plot with manually drawn illustrative linear boundaries that could roughly separate the four classes:</p> <p></p> <p>The dashed lines represent possible linear decision boundaries that a simple classifier could learn. A neural network could create smoother, curved separations in the overlapping regions.</p>"},{"location":"data/ex1/exercise1/#5-analysis","title":"5. Analysis","text":"<ol> <li> <p>Distribution and overlap:    Each class forms a distinct Gaussian cluster. The spread differs per class, producing varying degrees of overlap. The largest overlap occurs between Classes 0 and 1 because they are spatially closer and both elongated along the y-axis.</p> </li> <li> <p>Can a linear boundary separate all classes?    Almost. The data is mostly linearly separable \u2014 a combination of a few straight lines can correctly classify most points. However, perfect separation may require non-linear adjustment near overlapping regions.</p> </li> <li> <p>Expected neural network boundaries:    A neural network with non-linear activation (like tanh) would learn slightly curved or piecewise-linear boundaries. These adapt to subtle overlaps, refining the classification margins beyond what purely linear models can achieve.</p> </li> </ol>"},{"location":"data/ex1/exercise1/#6-conclusion","title":"6. Conclusion","text":"<p>This exercise demonstrates that even when data is mostly separable with straight lines, neural networks with non-linear activation can learn smoother and more flexible decision surfaces, improving performance where overlap or curvature exists.</p>"},{"location":"data/ex2/exercise2/","title":"Exercise 2 \u2014 Non-Linearity in Higher Dimensions","text":""},{"location":"data/ex2/exercise2/#1-dataset-generation","title":"1. Dataset Generation","text":"<p>In this exercise, we generated a dataset with 500 samples for Class A and 500 samples for Class B, using a multivariate normal distribution in 5 dimensions. The parameters for each class are given below:</p>"},{"location":"data/ex2/exercise2/#class-a","title":"Class A","text":"<p>Mean vector:</p> \\[ \\mu_A = [0, 0, 0, 0, 0] \\] <p>Covariance matrix:</p> \\[ \\Sigma_A = \\begin{bmatrix} 1.0 &amp; 0.8 &amp; 0.1 &amp; 0.0 &amp; 0.0 \\\\ 0.8 &amp; 1.0 &amp; 0.3 &amp; 0.0 &amp; 0.0 \\\\ 0.1 &amp; 0.3 &amp; 1.0 &amp; 0.5 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.5 &amp; 1.0 &amp; 0.2 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 1.0 \\end{bmatrix} \\]"},{"location":"data/ex2/exercise2/#class-b","title":"Class B","text":"<p>Mean vector:</p> \\[ \\mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] \\] <p>Covariance matrix:</p> \\[ \\Sigma_B = \\begin{bmatrix} 1.5 &amp; -0.7 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\\\ -0.7 &amp; 1.5 &amp; 0.4 &amp; 0.0 &amp; 0.0 \\\\ 0.2 &amp; 0.4 &amp; 1.5 &amp; 0.6 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.6 &amp; 1.5 &amp; 0.3 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.3 &amp; 1.5 \\end{bmatrix} \\]"},{"location":"data/ex2/exercise2/#2-dimensionality-reduction-and-visualization","title":"2. Dimensionality Reduction and Visualization","text":"<p>Since the dataset exists in 5 dimensions, we cannot visualize it directly. We use Principal Component Analysis (PCA) to reduce the data to 2 dimensions and visualize it in a scatter plot.</p> <p></p> <p>In this plot: - Blue points represent Class A - Orange points represent Class B</p> <p>The projection reveals that the two classes partially overlap in 2D space, forming complex boundaries that are not linearly separable.</p>"},{"location":"data/ex2/exercise2/#3-observations","title":"3. Observations","text":"<ol> <li> <p>Distribution and overlap:    The two classes overlap significantly after projection.    Although their 5D distributions differ, PCA compresses the most informative variance into 2D, making the overlap visible.    This overlap indicates that there is no single straight line (linear boundary) that can perfectly separate the classes.</p> </li> <li> <p>Linear separability:    The dataset is not linearly separable.    A simple perceptron or logistic regression model would struggle to classify the two classes correctly because their projections intertwine in multiple directions.</p> </li> <li> <p>Why a neural network is needed:    The relationships among features are non-linear \u2014 combinations of multiple dimensions interact in a way that a single linear function cannot capture.    A multi-layer neural network with non-linear activation functions (like ReLU or tanh) can map these complex interactions, effectively learning curved and flexible decision surfaces in the high-dimensional space.</p> </li> </ol>"},{"location":"data/ex2/exercise2/#4-example-visualization-of-decision-regions","title":"4. Example Visualization of Decision Regions","text":"<p>Below is an example (conceptual) of how a neural network could separate non-linear data after training:</p> <p></p> <p>The neural network learns curved boundaries that adapt to the geometry of the data, achieving much better separation than any linear model.</p> <p>Considering a linear nertwork, we would have as result:</p> <p></p>"},{"location":"data/ex2/exercise2/#5-conclusion","title":"5. Conclusion","text":"<p>This experiment demonstrates that: - Data that appears inseparable in lower-dimensional projections may actually be separable in higher dimensions. - Linear models are insufficient for capturing complex feature interactions. - Deep neural networks with non-linear activations can approximate these complex boundaries effectively.</p>"},{"location":"data/ex3/exercise3/","title":"Exercise 3 \u2014 Preparing Real-World Data for a Neural Network","text":""},{"location":"data/ex3/exercise3/#1-describe-the-data","title":"1. Describe the Data","text":""},{"location":"data/ex3/exercise3/#objective","title":"Objective","text":"<p>The Spaceship Titanic dataset simulates passenger records from a spaceship that suffered an accident. The goal is to predict whether each passenger was transported to another dimension, represented by the column:</p> <pre><code>\nTransported \u2192 Boolean (True/False)\n\n````\n\n- **True** \u2192 Passenger was transported.  \n- **False** \u2192 Passenger was not transported.  \n\nThis is a **binary classification** problem.\n\n---\n\n### Features\n\n| Type | Feature | Description |\n|------|----------|-------------|\n| **Categorical** | HomePlanet | Passenger\u2019s home planet (Earth, Europa, Mars) |\n| **Categorical** | CryoSleep | Whether the passenger was in cryosleep during the voyage |\n| **Categorical** | Cabin | Cabin identifier (deck/side/number) |\n| **Categorical** | Destination | Destination planet |\n| **Numerical** | Age | Passenger\u2019s age |\n| **Numerical** | RoomService, FoodCourt, ShoppingMall, Spa, VRDeck | Amount spent in each onboard amenity |\n| **Categorical** | Name | Passenger name (often not useful for modeling) |\n| **Categorical/Numerical** | PassengerId | Unique passenger identifier |\n| **Target** | Transported | Whether the passenger was transported (True/False) |\n\n---\n\n### Missing Values Investigation\n\nExample code:\n```python\nimport pandas as pd\ndf = pd.read_csv(\"spaceship_titanic.csv\")\nmissing = df.isnull().sum().sort_values(ascending=False)\nprint(missing[missing &gt; 0])\n````\n\nTypical results (approximate):\n\n| Column       | Missing Values |\n| ------------ | -------------- |\n| HomePlanet   | ~200           |\n| CryoSleep    | ~200           |\n| Cabin        | ~200           |\n| Destination  | ~180           |\n| Age          | ~180           |\n| RoomService  | ~180           |\n| FoodCourt    | ~200           |\n| ShoppingMall | ~210           |\n| Spa          | ~200           |\n| VRDeck       | ~190           |\n| Name         | ~100           |\n| Transported  | 0              |\n\nSeveral columns contain missing data, both numerical and categorical.\n\n---\n\n## 2. Preprocess the Data\n\n### a) Handle Missing Data\n\n**Strategy:**\n\n* **Numerical columns:** Replace missing values with the **median** (less sensitive to outliers).\n* **Categorical columns:** Replace missing values with the **mode** (most frequent value) or `\"Unknown\"`.\n\n```python\nnum_cols = ['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\ncat_cols = ['HomePlanet','CryoSleep','Cabin','Destination']\n\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\nfor c in cat_cols:\n    df[c] = df[c].fillna(df[c].mode()[0])\n</code></pre> <p>Justification: Using median/mode preserves all rows and avoids information loss \u2014 essential for stable neural network training.</p>"},{"location":"data/ex3/exercise3/#b-encode-categorical-features","title":"b) Encode Categorical Features","text":"<p>Convert non-numeric features into numeric form using One-Hot Encoding:</p> <pre><code>df = pd.get_dummies(df, columns=['HomePlanet','CryoSleep','Destination'], drop_first=True)\n</code></pre> <p>Example output columns:</p> <pre><code>HomePlanet_Europa, HomePlanet_Mars, CryoSleep_True, Destination_TRAPPIST-1e\n</code></pre> <p>Why: Neural networks require numeric inputs; one-hot encoding avoids false ordinal relationships.</p>"},{"location":"data/ex3/exercise3/#c-normalize-standardize-numerical-features","title":"c) Normalize / Standardize Numerical Features","text":"<p>Since the neural network uses tanh activation (range = [-1, 1]), inputs must be centered near zero. Use Standardization (mean = 0, std = 1) for stable gradients:</p> <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n</code></pre> <p>Why: <code>tanh</code> saturates for large values; scaling keeps inputs in its sensitive region, improving learning speed and convergence.</p>"},{"location":"data/ex3/exercise3/#3-visualize-the-results","title":"3. Visualize the Results","text":""},{"location":"data/ex3/exercise3/#example-histogram-before-and-after-scaling","title":"Example: Histogram Before and After Scaling","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(10,4))\ndf_raw = pd.read_csv(\"spaceship_titanic.csv\")\n\nax[0].hist(df_raw['Age'].dropna(), bins=30, edgecolor='k')\nax[0].set_title(\"Age before scaling\")\n\nax[1].hist(df['Age'], bins=30, edgecolor='k')\nax[1].set_title(\"Age after scaling (Standardized)\")\n\nplt.show()\n</code></pre> <p>Observation:</p> <ul> <li>Before scaling: values between 0\u201380, centered near 30.</li> <li>After scaling: mean \u2248 0, std \u2248 1 (range roughly -2 to +2).   This suits the <code>tanh</code> activation function\u2019s centered output range.</li> </ul>"},{"location":"data/ex3/exercise3/#4-summary","title":"4. Summary","text":"<p>Objective: Predict whether a passenger was transported (<code>Transported</code>).</p> <p>Steps Taken:</p> <ol> <li>Filled missing values (median/mode).</li> <li>One-Hot Encoded categorical variables.</li> <li>Standardized numeric columns for <code>tanh</code> activation.</li> <li>Visualized transformations via histograms.</li> </ol> <p>Why it matters: Proper preprocessing ensures clean, scaled, balanced inputs \u2014 key for stable neural network training and better accuracy.</p>"},{"location":"perceptron/exercise1/","title":"Exercise 1","text":""},{"location":"perceptron/exercise1/#data-generation-task","title":"Data Generation Task","text":"<p>In this exercise, we generated two classes of two-dimensional data points using multivariate normal distributions. Each class contains 1000 samples, with the following parameters:</p> <ul> <li>Class 0: mean = [1.5, 1.5], covariance matrix = [[0.5, 0], [0, 0.5]]  </li> <li>Class 1: mean = [5, 5], covariance matrix = [[0.5, 0], [0, 0.5]]</li> </ul> <p>These parameters ensure that both classes have low internal variability (standard deviation \u221a0.5 along each axis, no covariance between dimensions), resulting in relatively compact clusters. The significant distance between the two mean vectors creates a clear separation between the classes, with only minimal overlap.</p> <p>The plot below illustrates this separation: points from Class 0 are concentrated around (1.5, 1.5), while points from Class 1 are centered at (5, 5). Colors are used to highlight the difference, making it evident that the classes are mostly linearly separable.</p> <p></p>"},{"location":"perceptron/exercise1/#perceptron-implementation","title":"Perceptron Implementation","text":"<p>To classify the generated data, we implemented a single-layer perceptron from scratch, using only NumPy for basic linear algebra operations. The perceptron was initialized with a 2D weight vector and a bias term. Training followed the perceptron learning rule:  </p> <ul> <li>For each misclassified sample, update the weights and bias proportionally to the learning rate.  </li> <li>Training stops when no misclassifications occur in a full pass through the dataset (convergence) or after 100 epochs.  </li> <li>Accuracy was tracked after each epoch to monitor convergence.  </li> </ul>"},{"location":"perceptron/exercise1/#results","title":"Results","text":"<ul> <li>The perceptron converged very quickly due to the linear separability of the data.  </li> <li>Final accuracy was close to 100%, with very few or no misclassified points.  </li> <li>The decision boundary cleanly separated most data points, confirming that the perceptron is an effective linear classifier in this case.  </li> </ul> <p>Converged at epoch 12 Final weights: [0.19856224 0.17118283] Final bias: -1.2 Final accuracy: 1.0</p>"},{"location":"perceptron/exercise1/#visualization","title":"Visualization","text":"<p>The figure below shows the decision boundary overlaid on the data points. - Blue points correspond to Class 0 - Red points correspond to Class 1 - Yellow-circled points (if any) indicate misclassifications </p> <p></p> <p>Additionally, the training accuracy across epochs is plotted, illustrating the rapid convergence of the perceptron:  </p> <p></p>"},{"location":"perceptron/exercise1/#discussion","title":"Discussion","text":"<p>Because the two classes were generated with well-separated means and low variance, they are almost perfectly linearly separable. This allowed the perceptron to find a separating hyperplane within just a few epochs.  </p> <p>If the distributions had greater overlap, convergence would have taken longer, or the perceptron could have failed to reach 100% accuracy due to inherent non-linearity in the data.  </p>"},{"location":"perceptron/exercise2/","title":"Exercise 2 \u2014 Data Generation with Overlap and Perceptron Training","text":""},{"location":"perceptron/exercise2/#data-generation-task","title":"Data Generation Task","text":"<p>In this task, we generated two classes of 2D data points (1000 samples per class) using multivariate normal distributions. The parameters were chosen to create partial overlap between the classes:</p> <ul> <li>Class 0: mean = [2.5, 2.5], covariance matrix = [[1.5, 0], [0, 1.5]]  </li> <li>Class 1: mean = [4, 4], covariance matrix = [[1.5, 0], [0, 1.5]]  </li> </ul> <p>The larger variance along both dimensions (1.5) and the closer distance between the means result in clusters that are not fully linearly separable. This overlap makes classification more challenging compared to Exercise 1.  </p> <p>The plot below illustrates the generated data, showing visible regions of overlap:  </p> <p></p>"},{"location":"perceptron/exercise2/#perceptron-implementation-task","title":"Perceptron Implementation Task","text":"<p>We reused the single-layer perceptron implementation from Exercise 2, applying the same initialization, update rule, and training process.  </p> <ul> <li>The perceptron was trained for up to 100 epochs, or until convergence (no weight updates in a full pass).  </li> <li>Training accuracy was tracked at each epoch.  </li> <li>If the model did not converge, the final accuracy at 100 epochs was reported, and the possibility of oscillations was noted.  </li> <li>To reduce randomness, training was repeated across multiple runs with different initializations, and the best accuracy (or average across runs) was reported.  </li> </ul>"},{"location":"perceptron/exercise2/#results","title":"Results","text":"<ul> <li>Unlike in Exercise 2, the perceptron did not reach 100% accuracy due to the overlapping distributions.  </li> <li>Final accuracy stabilized below perfect classification (e.g., ~90\u201395%, depending on initialization).  </li> <li>The model occasionally oscillated in updates, as perfect linear separability is not possible.  </li> </ul> <p>The figure below shows the decision boundary found by the perceptron, overlaid on the data points:  </p> <p></p> <p>Misclassified points are highlighted with yellow markers:  </p> <p>The training accuracy over epochs is also shown, highlighting slower convergence and possible fluctuations:  </p> <p></p>"},{"location":"perceptron/exercise2/#discussion","title":"Discussion","text":"<p>The increased variance and closer means between the two classes introduced significant overlap in the feature space. As a result:  </p> <ul> <li>The perceptron could not fully separate the data, since a linear boundary cannot resolve all cases.  </li> <li>Training either converged to a suboptimal solution or oscillated between weight updates.  </li> <li>Accuracy plateaued below 100%, typically in the 90\u201395% range, reflecting the inherent non-separability.  </li> </ul> <p>This demonstrates a key limitation of the perceptron: while effective on linearly separable datasets, it struggles when overlap or non-linear boundaries are present. In such cases, more advanced models (e.g., multi-layer neural networks, kernel methods) are required.  </p>"}]}